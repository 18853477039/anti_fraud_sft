{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30c94524-d8c9-4ca3-aa6a-ceff060d39c8",
   "metadata": {},
   "source": [
    "## 引言\n",
    "前文[微调方法概览](https://golfxiao.blog.csdn.net/article/details/140859269)总结了微调的各种方法，并且在更前面两篇文章[欺诈文本分类微调（六）：Lora单卡](https://golfxiao.blog.csdn.net/article/details/141440847)\n",
    "和[欺诈文本分类微调（七）—— lora单卡二次调优](https://golfxiao.blog.csdn.net/article/details/141500352)中已经尝试过用Lora进行微调，本文出于好奇准备尝试下用QLora进行微调的效果。\n",
    "\n",
    "QLoRA是一种新的微调大型语言模型（LLM）的方法，它的特点是能在节省内存的同时保持推理性能。它的出现是为了应对大型模型微调时内存需求大，成本昂贵的问题。\n",
    "\n",
    "工作原理：首先将LLM进行4位量化，从而显著减少模型的内存占用，接着使用低阶适配器（LoRA）方法对量化的LLM进行微调，因此，QLora可以看成是量化+Lora的结合体。\n",
    "\n",
    "具有以下核心技术：\n",
    "- 4位量化，它创新性的引入了特殊的4位浮点数表示方法NF4(Normal Float 4-bit)，使用非均匀量化来平衡数据范围与精度。\n",
    "- 双量化，一种对量化后常数再次进行量化的方法，每个参数可平均节省约 0.37 位。\n",
    "- 分页优化，使用具有 NVIDIA 统一内存的分页优化器，以避免具有长序列长度的小批量时出现内存峰值。1\n",
    "\n",
    "\n",
    "\n",
    "依赖库安装：\n",
    "```\n",
    "pip install -q -U bitsandbytes\n",
    "\n",
    "> bitsandbytes主要是针对llm和transformers模型提供了优化和量化模型的功能，专门为8位优化器、矩阵乘法和量化而设计，提供了像8位Adam/AdamW之类的函数。目标是通过8位操作实现高效的计算和内存使用从而使llm更易于访问。```\n",
    "\n",
    "本文的用欺诈文本分类这个业务场景来目的是实际验证下QLora进行量化微调。效果，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5fb303-fde9-4384-90ac-ef36d3f2ef1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dcfed9ad-1675-4fc5-a0cb-e5bee9373c43",
   "metadata": {},
   "source": [
    "## QLora训练\n",
    "\n",
    "#### 初始化\n",
    "引入之前封装好的trainer.py脚本，定义模型路径和数据集路径，以及要使用的GPU设备。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "690f17e1-5810-4dd7-9ee0-066a3f398705",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ce5eaa4-db11-4f8f-8f5f-8db85df6dc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_path = '/data2/anti_fraud/dataset/train0819.jsonl'\n",
    "evaldata_path = '/data2/anti_fraud/dataset/eval0819.jsonl'\n",
    "model_path = '/data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct'\n",
    "output_path = '/data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0830_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "470d2cd0-781c-440c-827e-2ed5dc913141",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0aa43b-8a59-4005-95f3-cde710b8e36f",
   "metadata": {},
   "source": [
    "#### 加载模型和数据集\n",
    "\n",
    "由于QLora需要以量化的方式来加载模型，所以加载模型的方法需要作调整，这里的改动是引入`BitsAndBytesConfig`类构建一个量化配置`quantization_config`, 具体配置项释义：\n",
    "- load_in_4bit：决定了模型参数以4位量化格式加载，加载后的模型参数占用空间会比较小； \n",
    "- bnb_4bit_compute_dtype=bfloat16：决定了矩阵乘法的计算精度使用bfloat16，输入数据也会被转换成bfloat16位进行计算； \n",
    "- bnb_4bit_quant_type：指定量化数据类型nf4； \n",
    "- bnb_4bit_use_double_quant：是否启用双重量化； "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f29089dd-d304-42ba-bdde-a2ca5821cc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "def load_model(model_path, device='cuda'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        quantization_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_use_double_quant=False, \n",
    "            bnb_4bit_quant_type='nf4'\n",
    "        ),\n",
    "    )\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7751b46a-6f2e-4b1c-b775-04fc49c65e74",
   "metadata": {},
   "source": [
    "> 注1：普通的量化通常是将数值分成均匀的区间，比如，将0到1之间的数值分成16个区间，每个区间的宽度相同。而NF4则根据数据的分布情况，使用不均匀的区间来表示数值，这样可以更有效地表示模型中的重要数值，特别是那些频繁出现的数值。\n",
    "\n",
    "> 注2：双重量化是指在已经量化的基础上再进行量化，第二次量化并不会改变位数本身（即仍然是4位），它的目的是通过更紧凑地表示数值，使得存储和计算更加高效。由于每一次量化都会引入一些量化误差，双重量化可能会带来更大的数值误差，所以一般只用于极端内存受限的情况下。\n",
    "\n",
    "> 注3：之所以模型参数加载使用4位而计算时使用16位，是因为量化本身已经带来了误差，计算时需要采用更高的精度是为了减少量化误差带来的影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba45923-3ae9-4274-a6b9-39040cff913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "加载模型参数和token序列化器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9dc72f7-13c4-4b52-a0a7-e7678da1422c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.14 s, sys: 574 ms, total: 1.72 s\n",
      "Wall time: 1.81 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model, tokenizer = load_model(model_path, device)\n",
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce86a0d4-3d5a-44fe-b90e-689c2b0fb332",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '，' (U+FF0C) (3210621457.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    > 通过统计的耗时信息可以看到，使用量化方式加载模型的过程是比较慢的，耗时了1分52秒，中间涉及到将模型参数从高精度的浮点数（如 FP32）转换为低精度的 NF4 格式。与之对比，不带量化时基本是秒加载。\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '，' (U+FF0C)\n"
     ]
    }
   ],
   "source": [
    "> 通过统计的耗时信息可以看到，使用量化方式加载模型的过程是比较慢的，耗时了1分52秒，中间涉及到将模型参数从高精度的浮点数（如 FP32）转换为低精度的 NF4 格式。与之对比，不带量化时基本是秒加载。\n",
    "\n",
    "加载数据集，复用前文的方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a063d35-fb6e-4eef-bcd2-5fad80ca0281",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, eval_dataset = load_dataset(traindata_path, evaldata_path, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e85e14d-fd55-4444-b95e-44060c77ca73",
   "metadata": {},
   "source": [
    "#### 构建训练参数\n",
    "\n",
    "训练参数：引入分页内存优化器来优化训练过程中的内存分配。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d76fd7f-b5fa-424f-8bf6-f0f93bd827c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = build_train_arguments(output_path)\n",
    "train_args.optim=\"paged_adamw_32bit\"   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2513791-cdc3-4e17-ba35-353eb37f74e5",
   "metadata": {},
   "source": [
    "> paged_adamw_32bit 是一种优化器配置，它使用了分页内存管理和32位浮点数来优化训练过程，可以帮助你在训练大规模模型时更有效地管理内存和计算资源。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3d896a-15ce-406f-a994-278383f219c7",
   "metadata": {},
   "source": [
    "lora配置：同[前文Lora训练](https://golfxiao.blog.csdn.net/article/details/141500352)一样使用大小为16的秩。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "319ecde9-4c5d-4abf-82d3-1b7750c475db",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = build_loraconfig()\n",
    "lora_config.lora_dropout = 0.2   \n",
    "lora_config.r = 16\n",
    "lora_config.lora_alpha = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fd320f-37e9-47a6-9c34-651dc6cce658",
   "metadata": {},
   "source": [
    "#### 开始训练\n",
    "\n",
    "构造训练器开始训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40426a90-7120-45ba-b80f-debff1aea902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 18,464,768 || all params: 1,562,179,072 || trainable%: 1.1820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1900' max='3522' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1900/3522 39:42 < 33:56, 0.80 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.046700</td>\n",
       "      <td>0.029243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.041400</td>\n",
       "      <td>0.041580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.016300</td>\n",
       "      <td>0.025357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.027700</td>\n",
       "      <td>0.022728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.023300</td>\n",
       "      <td>0.022268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.028900</td>\n",
       "      <td>0.021204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.019347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>0.022994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>0.019503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.016300</td>\n",
       "      <td>0.018722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.019900</td>\n",
       "      <td>0.018770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.018239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.023348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.016903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>0.018311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.018219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.017595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>0.017683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.014400</td>\n",
       "      <td>0.016938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1900, training_loss=0.035359279932944396, metrics={'train_runtime': 2384.4155, 'train_samples_per_second': 23.637, 'train_steps_per_second': 1.477, 'total_flos': 8.369815519690752e+16, 'train_loss': 0.035359279932944396, 'epoch': 1.61839863713799})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = build_trainer(model, tokenizer, train_args, lora_config, train_dataset, eval_dataset)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1d4cc2-f7f2-46aa-8b23-4b9142d5237d",
   "metadata": {},
   "outputs": [],
   "source": [
    "训练过程中观察内存变化：22G+。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eeda677-faa6-4749-ae72-71643c6cdcc0",
   "metadata": {},
   "source": [
    "训练时的显存占用相比非量化时并没有明显变化，基本上占满了24G显卡的显存。\n",
    "\n",
    "推测原因可能是：QLoRA只是通过量化技术减少了模型参数加载时的显存占用，但训练时仍然会反量化为16位进行矩阵计算，尤其是前向和反向传播阶段，显存的主要消耗来自于激活值、梯度和优化器状态，模型参数仅仅是一小部分，这就导致真正训练过程中占用的显存相比非量化时并没有减少。\n",
    "\n",
    "QLoRA 主要通过量化模型参数来减小显存占用，但在需要更大的 batch size 的场景下，其显存优化效果可能并不显著。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2de9cc2-d737-4d73-ac38-b5dfb90a3ad5",
   "metadata": {},
   "source": [
    "## 评估测试\n",
    "用验证损失最低的checkpoint-2200进行测评。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf5f6ba4-c887-4244-ae77-1b576818a4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress: 100%|██████████| 2348/2348 [02:31<00:00, 15.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn：1143, fp:22, fn:215, tp:968\n",
      "precision: 0.9777777777777777, recall: 0.8182586644125106, accuracy: 0.8990630323679727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%run evaluate.py\n",
    "checkpoint_path='/data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0830_2/checkpoint-1700'\n",
    "evaluate(model_path, checkpoint_path, evaldata_path, device, batch=True, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde50c6f-5269-481c-b052-c1489e4475c8",
   "metadata": {},
   "source": [
    "这个训练结果和前文的召回率`0.86`区别不大，说明使用量化后的模型参数进行训练确实能保持和16位精度的参数训练几乎一样的效果。\n",
    "\n",
    "**小结**：本文通过实际训练来测试QLora对于显存占用和推理性能方面的效果，在我们这个验证结果里，推理性能方面几乎可以保持同先前一样的效果，但显存占用只在加载时降低了不到1/2, 而在训练过程中相比于非量化时没有明显减少。原因可能是由于我们的模型太小，训练时的批量相对较大，所以模型参数加载时所优化的那部分内存，与整个训练过程中所占用的内存相比比例较低，内存优化的整体效果就不明显。基于QLora主要是通过减少模型参数所占用的显存这个原理出发，个人理解可能在参数更大的模型和batch_size更小的训练时效果可能会有所提升。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1b5378-78e5-46d1-9b27-438c9f3637bc",
   "metadata": {},
   "source": [
    "## 相关阅读\n",
    "- [微调方法概览](https://golfxiao.blog.csdn.net/article/details/140859269)\n",
    "- [欺诈文本分类微调（六)：Lora单卡训练](https://golfxiao.blog.csdn.net/article/details/141440847)\n",
    "- [欺诈文本分类微调（七)：lora单卡二次调优](https://golfxiao.blog.csdn.net/article/details/141500352)\n",
    "- [QLoRA量化微调策略与实践](https://blog.csdn.net/FrenzyTechAI/article/details/132686051)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
