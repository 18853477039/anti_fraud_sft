{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42fb0933-361f-4698-8cd1-5712625ed710",
   "metadata": {},
   "source": [
    "## 背景\n",
    "随着网络诈骗越来越多，经常有一些不法分子利用网络会议软件进行诈骗，为此需要训练一个文本分类检测模型，来实时检测会议中的对话内容是否存在诈骗风险，以帮助用户在网络会议中增强警惕，减少受到欺诈的风险。\n",
    "\n",
    "考虑模型的预训练成本高昂，选择一个通用能力强的模型底座进行微调是一个比较经济的做法，我们选择模型底座主要考虑以下几个因素：\n",
    "- 预训练主要采用中文语料，具有良好的中文支持能力\n",
    "- 模型需要具备基本的指令遵循能力。\n",
    "- 模型要能理解json格式，具备输出json格式的能力。\n",
    "- 在满足以上几个能力的基础上，模型参数量越小越好。\n",
    "\n",
    "选厂商：\n",
    "- 智谱AI：清华，参数规模主要在6B、9B\n",
    "- 书生浦语：商汤，只有一个7B规模的参数\n",
    "- 零一万物：创新工厂，只有一个6B模型\n",
    "- 通义千问：阿里，参数规模有0.5B、1.5B、7B、72B\n",
    "\n",
    "通义千问Qwen2提供了不同参数大小的模型，能让我们有更多的选择。我们需要做的是从大到小依次测试每个模型的能力，找到满足自己需要的最小参数模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7876875-4400-41c9-a172-3e0398553150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ac677a-5abf-48e2-812b-6c4c57721c5f",
   "metadata": {},
   "source": [
    "## 模型下载\n",
    "\n",
    "依次下载qwen2的不同尺寸的模型：0.5B-Instruct、1.5B、1.5B-Instruct、7B-Instruct，下载完后会输出模型在本地磁盘的路径。\n",
    "> 1.5B和1.5B-Instruct的主要区别：按照官方文档，前者是预训练模型，后者是经过指令微调的模型。为了确定应该选择预训练模型还是指令微调模型，这里将两个模型都下载下来进行对比测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bb7b7ca-d872-4572-9786-f3e10c760f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model to directory: /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1.5B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 02:35:29,020 - modelscope - WARNING - Using branch: master as version is unstable, use with caution\n",
      "2025-03-02 02:35:29,262 - modelscope - INFO - Got 10 files, start to download ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e5b7eb6931647729059e982ad200003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing 10 items:   0%|          | 0.00/10.0 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef2b11422614c2b984372e12ce8315c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [tokenizer.json]:   0%|          | 0.00/6.70M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbbc3157aa4340a881433bfc980ba1f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [configuration.json]:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d11e31c698417888c382c91681bb67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [config.json]:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e1225cad868479284ced7097f1605f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [generation_config.json]:   0%|          | 0.00/138 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a6af63237d42fcb71d4f53308eac05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [model.safetensors]:   0%|          | 0.00/2.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b6be6784cbc4c96919c5a31e3a65589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [merges.txt]:   0%|          | 0.00/1.59M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628135b08db6468589744af043739225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [LICENSE]:   0%|          | 0.00/11.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98e2690e9f74d9fb0d7f0683edc169a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [README.md]:   0%|          | 0.00/4.71k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c08396cc6a44b1904ad2eab6f43978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [tokenizer_config.json]:   0%|          | 0.00/1.26k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a091ed421e88426c91a234c1c9959675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [vocab.json]:   0%|          | 0.00/2.65M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dd785928b004c248dd7528012af5224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading [model.safetensors]:   0%|          | 0.00/2.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 02:45:12,118 - modelscope - INFO - Download model 'Qwen/Qwen2-1.5B' successfully.\n",
      "2025-03-02 02:45:12,119 - modelscope - INFO - Creating symbolic link [/data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1.5B].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#模型下载\n",
    "from modelscope import snapshot_download\n",
    "cache_dir = '/root/autodl-fs/data2/anti_fraud/models/modelscope/hub/hub/'\n",
    "# model_dir = snapshot_download('Qwen/Qwen2-7B-Instruct', cache_dir=cache_dir, revision='master')\n",
    "# model_dir = snapshot_download('Qwen/Qwen2-0.5B-Instruct', cache_dir=cache_dir, revision='master')\n",
    "# model_dir = snapshot_download('Qwen/Qwen2-1.5B-Instruct', cache_dir=cache_dir, revision='master')\n",
    "model_dir = snapshot_download('Qwen/Qwen2-1.5B', cache_dir=cache_dir, revision='master')\n",
    "model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6f47306-152a-4eca-8f41-3db74f13057c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 14885547\n",
      "-rw-r--r-- 1 root root      11344 Mar 13 21:35 LICENSE\n",
      "-rw-r--r-- 1 root root       6577 Mar 13 21:39 README.md\n",
      "-rw-r--r-- 1 root root        663 Mar 13 21:37 config.json\n",
      "-rw-r--r-- 1 root root         48 Mar 13 21:35 configuration.json\n",
      "-rw-r--r-- 1 root root        243 Mar 13 21:37 generation_config.json\n",
      "-rw-r--r-- 1 root root    1671839 Mar 13 21:35 merges.txt\n",
      "-rw-r--r-- 1 root root 3945426872 Mar 13 21:37 model-00001-of-00004.safetensors\n",
      "-rw-r--r-- 1 root root 3864726352 Mar 13 21:35 model-00002-of-00004.safetensors\n",
      "-rw-r--r-- 1 root root 3864726408 Mar 13 21:38 model-00003-of-00004.safetensors\n",
      "-rw-r--r-- 1 root root 3556392240 Mar 13 21:39 model-00004-of-00004.safetensors\n",
      "-rw-r--r-- 1 root root      27752 Mar 13 21:39 model.safetensors.index.json\n",
      "-rw-r--r-- 1 root root    7028015 Mar 13 21:39 tokenizer.json\n",
      "-rw-r--r-- 1 root root       1288 Mar 13 21:39 tokenizer_config.json\n",
      "-rw-r--r-- 1 root root    2776833 Mar 13 21:40 vocab.json\n"
     ]
    }
   ],
   "source": [
    "!ls -l '/root/autodl-fs/data2/anti_fraud/models/modelscope/hub/hub/Qwen/Qwen2-7B-Instruct'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc13f5f-8be8-43b2-9c22-a3e20aed15f6",
   "metadata": {},
   "source": [
    "## 封装函数\n",
    "\n",
    "先封装一个函数load_model，用于从参数model_dir指定的路径中加载模型model和序列化器tokenizer，加载完后将模型手动移动到指定的GPU设备上。\n",
    "\n",
    "> 注：如果同时指定device和device_map=“auto\"，可能会导致RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "773028f0-77bb-4170-a3d6-3fa4d6543c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def load_model(model_dir, device='cuda'):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_dir,\n",
    "        torch_dtype=\"auto\",\n",
    "        trust_remote_code=True\n",
    "        # device_map=\"auto\"     \n",
    "    )\n",
    "    \n",
    "    model = model.to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145f6732-dfdb-4c54-bd50-af345e21cc71",
   "metadata": {},
   "source": [
    "再封装一个predict函数用于文本推理，考虑到我们将要用多个不同参数的模型分别进行测试，这里将model和tokenizer提取到参数中，以便复用这个方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3387f6e7-3335-4460-af13-cf0d70273eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, tokenizer, prompt, device='cuda', debug=True):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    print(f\"input: {text}\") if debug else None\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "    print(f\"input_ids: {model_inputs}\") if debug else None\n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=512,\n",
    "    )\n",
    "    print(f\"generated_ids: {generated_ids}\") if debug else None\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9323f543-f4ed-4578-98af-270d55b1fe2a",
   "metadata": {},
   "source": [
    "## 7B-Instruct模型测试\n",
    "\n",
    "首先加载模型，并指定模型要加载到的目标设备。\n",
    "\n",
    "> cuda:2表示序号为2的GPU设备，当机器上有多张GPU时，用序号来区分不同的GPU，序号从0开始。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9726e4a6-684d-44df-83f4-a95641466d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a17cbb4399b94b908e3368c56a1fcb71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda:0\" # the device to load the model onto\n",
    "\n",
    "model_dir = \"/root/autodl-fs/data2/anti_fraud/models/modelscope/hub/hub/Qwen/Qwen2-7B-Instruct\"\n",
    "\n",
    "model70, tokenizer70 = load_model(model_dir, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393222c5-20a0-404c-9314-62d6528234dc",
   "metadata": {},
   "source": [
    "7B模型使用transformers加载大概占用内存15G左右，\n",
    "```\n",
    " 2   N/A  N/A     38755      C   ...naconda3/envs/python3_10/bin/python    15392MiB\n",
    "```\n",
    "\n",
    "检查模型参数的设备分配是否如预期。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f66ac9d-f61a-456a-9e8e-e2499a7a941a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight cuda:0\n",
      "model.layers.0.self_attn.q_proj.weight cuda:0\n",
      "model.layers.0.self_attn.q_proj.bias cuda:0\n",
      "model.layers.0.self_attn.k_proj.weight cuda:0\n",
      "model.layers.0.self_attn.k_proj.bias cuda:0\n",
      "model.layers.0.self_attn.v_proj.weight cuda:0\n",
      "model.layers.0.self_attn.v_proj.bias cuda:0\n",
      "model.layers.0.self_attn.o_proj.weight cuda:0\n",
      "model.layers.0.mlp.gate_proj.weight cuda:0\n",
      "model.layers.0.mlp.up_proj.weight cuda:0\n",
      "model.layers.0.mlp.down_proj.weight cuda:0\n",
      "model.layers.0.input_layernorm.weight cuda:0\n",
      "model.layers.0.post_attention_layernorm.weight cuda:0\n",
      "model.layers.1.self_attn.q_proj.weight cuda:0\n",
      "model.layers.1.self_attn.q_proj.bias cuda:0\n",
      "model.layers.1.self_attn.k_proj.weight cuda:0\n",
      "model.layers.1.self_attn.k_proj.bias cuda:0\n",
      "model.layers.1.self_attn.v_proj.weight cuda:0\n",
      "model.layers.1.self_attn.v_proj.bias cuda:0\n",
      "model.layers.1.self_attn.o_proj.weight cuda:0\n",
      "model.layers.1.mlp.gate_proj.weight cuda:0\n",
      "model.layers.1.mlp.up_proj.weight cuda:0\n",
      "model.layers.1.mlp.down_proj.weight cuda:0\n",
      "model.layers.1.input_layernorm.weight cuda:0\n",
      "model.layers.1.post_attention_layernorm.weight cuda:0\n",
      "model.layers.2.self_attn.q_proj.weight cuda:0\n",
      "model.layers.2.self_attn.q_proj.bias cuda:0\n",
      "model.layers.2.self_attn.k_proj.weight cuda:0\n",
      "model.layers.2.self_attn.k_proj.bias cuda:0\n",
      "model.layers.2.self_attn.v_proj.weight cuda:0\n",
      "model.layers.2.self_attn.v_proj.bias cuda:0\n",
      "model.layers.2.self_attn.o_proj.weight cuda:0\n",
      "model.layers.2.mlp.gate_proj.weight cuda:0\n",
      "model.layers.2.mlp.up_proj.weight cuda:0\n",
      "model.layers.2.mlp.down_proj.weight cuda:0\n",
      "model.layers.2.input_layernorm.weight cuda:0\n",
      "model.layers.2.post_attention_layernorm.weight cuda:0\n",
      "model.layers.3.self_attn.q_proj.weight cuda:0\n",
      "model.layers.3.self_attn.q_proj.bias cuda:0\n",
      "model.layers.3.self_attn.k_proj.weight cuda:0\n",
      "model.layers.3.self_attn.k_proj.bias cuda:0\n",
      "model.layers.3.self_attn.v_proj.weight cuda:0\n",
      "model.layers.3.self_attn.v_proj.bias cuda:0\n",
      "model.layers.3.self_attn.o_proj.weight cuda:0\n",
      "model.layers.3.mlp.gate_proj.weight cuda:0\n",
      "model.layers.3.mlp.up_proj.weight cuda:0\n",
      "model.layers.3.mlp.down_proj.weight cuda:0\n",
      "model.layers.3.input_layernorm.weight cuda:0\n",
      "model.layers.3.post_attention_layernorm.weight cuda:0\n",
      "model.layers.4.self_attn.q_proj.weight cuda:0\n",
      "model.layers.4.self_attn.q_proj.bias cuda:0\n",
      "model.layers.4.self_attn.k_proj.weight cuda:0\n",
      "model.layers.4.self_attn.k_proj.bias cuda:0\n",
      "model.layers.4.self_attn.v_proj.weight cuda:0\n",
      "model.layers.4.self_attn.v_proj.bias cuda:0\n",
      "model.layers.4.self_attn.o_proj.weight cuda:0\n",
      "model.layers.4.mlp.gate_proj.weight cuda:0\n",
      "model.layers.4.mlp.up_proj.weight cuda:0\n",
      "model.layers.4.mlp.down_proj.weight cuda:0\n",
      "model.layers.4.input_layernorm.weight cuda:0\n",
      "model.layers.4.post_attention_layernorm.weight cuda:0\n",
      "model.layers.5.self_attn.q_proj.weight cuda:0\n",
      "model.layers.5.self_attn.q_proj.bias cuda:0\n",
      "model.layers.5.self_attn.k_proj.weight cuda:0\n",
      "model.layers.5.self_attn.k_proj.bias cuda:0\n",
      "model.layers.5.self_attn.v_proj.weight cuda:0\n",
      "model.layers.5.self_attn.v_proj.bias cuda:0\n",
      "model.layers.5.self_attn.o_proj.weight cuda:0\n",
      "model.layers.5.mlp.gate_proj.weight cuda:0\n",
      "model.layers.5.mlp.up_proj.weight cuda:0\n",
      "model.layers.5.mlp.down_proj.weight cuda:0\n",
      "model.layers.5.input_layernorm.weight cuda:0\n",
      "model.layers.5.post_attention_layernorm.weight cuda:0\n",
      "model.layers.6.self_attn.q_proj.weight cuda:0\n",
      "model.layers.6.self_attn.q_proj.bias cuda:0\n",
      "model.layers.6.self_attn.k_proj.weight cuda:0\n",
      "model.layers.6.self_attn.k_proj.bias cuda:0\n",
      "model.layers.6.self_attn.v_proj.weight cuda:0\n",
      "model.layers.6.self_attn.v_proj.bias cuda:0\n",
      "model.layers.6.self_attn.o_proj.weight cuda:0\n",
      "model.layers.6.mlp.gate_proj.weight cuda:0\n",
      "model.layers.6.mlp.up_proj.weight cuda:0\n",
      "model.layers.6.mlp.down_proj.weight cuda:0\n",
      "model.layers.6.input_layernorm.weight cuda:0\n",
      "model.layers.6.post_attention_layernorm.weight cuda:0\n",
      "model.layers.7.self_attn.q_proj.weight cuda:0\n",
      "model.layers.7.self_attn.q_proj.bias cuda:0\n",
      "model.layers.7.self_attn.k_proj.weight cuda:0\n",
      "model.layers.7.self_attn.k_proj.bias cuda:0\n",
      "model.layers.7.self_attn.v_proj.weight cuda:0\n",
      "model.layers.7.self_attn.v_proj.bias cuda:0\n",
      "model.layers.7.self_attn.o_proj.weight cuda:0\n",
      "model.layers.7.mlp.gate_proj.weight cuda:0\n",
      "model.layers.7.mlp.up_proj.weight cuda:0\n",
      "model.layers.7.mlp.down_proj.weight cuda:0\n",
      "model.layers.7.input_layernorm.weight cuda:0\n",
      "model.layers.7.post_attention_layernorm.weight cuda:0\n",
      "model.layers.8.self_attn.q_proj.weight cuda:0\n",
      "model.layers.8.self_attn.q_proj.bias cuda:0\n",
      "model.layers.8.self_attn.k_proj.weight cuda:0\n",
      "model.layers.8.self_attn.k_proj.bias cuda:0\n",
      "model.layers.8.self_attn.v_proj.weight cuda:0\n",
      "model.layers.8.self_attn.v_proj.bias cuda:0\n",
      "model.layers.8.self_attn.o_proj.weight cuda:0\n",
      "model.layers.8.mlp.gate_proj.weight cuda:0\n",
      "model.layers.8.mlp.up_proj.weight cuda:0\n",
      "model.layers.8.mlp.down_proj.weight cuda:0\n",
      "model.layers.8.input_layernorm.weight cuda:0\n",
      "model.layers.8.post_attention_layernorm.weight cuda:0\n",
      "model.layers.9.self_attn.q_proj.weight cuda:0\n",
      "model.layers.9.self_attn.q_proj.bias cuda:0\n",
      "model.layers.9.self_attn.k_proj.weight cuda:0\n",
      "model.layers.9.self_attn.k_proj.bias cuda:0\n",
      "model.layers.9.self_attn.v_proj.weight cuda:0\n",
      "model.layers.9.self_attn.v_proj.bias cuda:0\n",
      "model.layers.9.self_attn.o_proj.weight cuda:0\n",
      "model.layers.9.mlp.gate_proj.weight cuda:0\n",
      "model.layers.9.mlp.up_proj.weight cuda:0\n",
      "model.layers.9.mlp.down_proj.weight cuda:0\n",
      "model.layers.9.input_layernorm.weight cuda:0\n",
      "model.layers.9.post_attention_layernorm.weight cuda:0\n",
      "model.layers.10.self_attn.q_proj.weight cuda:0\n",
      "model.layers.10.self_attn.q_proj.bias cuda:0\n",
      "model.layers.10.self_attn.k_proj.weight cuda:0\n",
      "model.layers.10.self_attn.k_proj.bias cuda:0\n",
      "model.layers.10.self_attn.v_proj.weight cuda:0\n",
      "model.layers.10.self_attn.v_proj.bias cuda:0\n",
      "model.layers.10.self_attn.o_proj.weight cuda:0\n",
      "model.layers.10.mlp.gate_proj.weight cuda:0\n",
      "model.layers.10.mlp.up_proj.weight cuda:0\n",
      "model.layers.10.mlp.down_proj.weight cuda:0\n",
      "model.layers.10.input_layernorm.weight cuda:0\n",
      "model.layers.10.post_attention_layernorm.weight cuda:0\n",
      "model.layers.11.self_attn.q_proj.weight cuda:0\n",
      "model.layers.11.self_attn.q_proj.bias cuda:0\n",
      "model.layers.11.self_attn.k_proj.weight cuda:0\n",
      "model.layers.11.self_attn.k_proj.bias cuda:0\n",
      "model.layers.11.self_attn.v_proj.weight cuda:0\n",
      "model.layers.11.self_attn.v_proj.bias cuda:0\n",
      "model.layers.11.self_attn.o_proj.weight cuda:0\n",
      "model.layers.11.mlp.gate_proj.weight cuda:0\n",
      "model.layers.11.mlp.up_proj.weight cuda:0\n",
      "model.layers.11.mlp.down_proj.weight cuda:0\n",
      "model.layers.11.input_layernorm.weight cuda:0\n",
      "model.layers.11.post_attention_layernorm.weight cuda:0\n",
      "model.layers.12.self_attn.q_proj.weight cuda:0\n",
      "model.layers.12.self_attn.q_proj.bias cuda:0\n",
      "model.layers.12.self_attn.k_proj.weight cuda:0\n",
      "model.layers.12.self_attn.k_proj.bias cuda:0\n",
      "model.layers.12.self_attn.v_proj.weight cuda:0\n",
      "model.layers.12.self_attn.v_proj.bias cuda:0\n",
      "model.layers.12.self_attn.o_proj.weight cuda:0\n",
      "model.layers.12.mlp.gate_proj.weight cuda:0\n",
      "model.layers.12.mlp.up_proj.weight cuda:0\n",
      "model.layers.12.mlp.down_proj.weight cuda:0\n",
      "model.layers.12.input_layernorm.weight cuda:0\n",
      "model.layers.12.post_attention_layernorm.weight cuda:0\n",
      "model.layers.13.self_attn.q_proj.weight cuda:0\n",
      "model.layers.13.self_attn.q_proj.bias cuda:0\n",
      "model.layers.13.self_attn.k_proj.weight cuda:0\n",
      "model.layers.13.self_attn.k_proj.bias cuda:0\n",
      "model.layers.13.self_attn.v_proj.weight cuda:0\n",
      "model.layers.13.self_attn.v_proj.bias cuda:0\n",
      "model.layers.13.self_attn.o_proj.weight cuda:0\n",
      "model.layers.13.mlp.gate_proj.weight cuda:0\n",
      "model.layers.13.mlp.up_proj.weight cuda:0\n",
      "model.layers.13.mlp.down_proj.weight cuda:0\n",
      "model.layers.13.input_layernorm.weight cuda:0\n",
      "model.layers.13.post_attention_layernorm.weight cuda:0\n",
      "model.layers.14.self_attn.q_proj.weight cuda:0\n",
      "model.layers.14.self_attn.q_proj.bias cuda:0\n",
      "model.layers.14.self_attn.k_proj.weight cuda:0\n",
      "model.layers.14.self_attn.k_proj.bias cuda:0\n",
      "model.layers.14.self_attn.v_proj.weight cuda:0\n",
      "model.layers.14.self_attn.v_proj.bias cuda:0\n",
      "model.layers.14.self_attn.o_proj.weight cuda:0\n",
      "model.layers.14.mlp.gate_proj.weight cuda:0\n",
      "model.layers.14.mlp.up_proj.weight cuda:0\n",
      "model.layers.14.mlp.down_proj.weight cuda:0\n",
      "model.layers.14.input_layernorm.weight cuda:0\n",
      "model.layers.14.post_attention_layernorm.weight cuda:0\n",
      "model.layers.15.self_attn.q_proj.weight cuda:0\n",
      "model.layers.15.self_attn.q_proj.bias cuda:0\n",
      "model.layers.15.self_attn.k_proj.weight cuda:0\n",
      "model.layers.15.self_attn.k_proj.bias cuda:0\n",
      "model.layers.15.self_attn.v_proj.weight cuda:0\n",
      "model.layers.15.self_attn.v_proj.bias cuda:0\n",
      "model.layers.15.self_attn.o_proj.weight cuda:0\n",
      "model.layers.15.mlp.gate_proj.weight cuda:0\n",
      "model.layers.15.mlp.up_proj.weight cuda:0\n",
      "model.layers.15.mlp.down_proj.weight cuda:0\n",
      "model.layers.15.input_layernorm.weight cuda:0\n",
      "model.layers.15.post_attention_layernorm.weight cuda:0\n",
      "model.layers.16.self_attn.q_proj.weight cuda:0\n",
      "model.layers.16.self_attn.q_proj.bias cuda:0\n",
      "model.layers.16.self_attn.k_proj.weight cuda:0\n",
      "model.layers.16.self_attn.k_proj.bias cuda:0\n",
      "model.layers.16.self_attn.v_proj.weight cuda:0\n",
      "model.layers.16.self_attn.v_proj.bias cuda:0\n",
      "model.layers.16.self_attn.o_proj.weight cuda:0\n",
      "model.layers.16.mlp.gate_proj.weight cuda:0\n",
      "model.layers.16.mlp.up_proj.weight cuda:0\n",
      "model.layers.16.mlp.down_proj.weight cuda:0\n",
      "model.layers.16.input_layernorm.weight cuda:0\n",
      "model.layers.16.post_attention_layernorm.weight cuda:0\n",
      "model.layers.17.self_attn.q_proj.weight cuda:0\n",
      "model.layers.17.self_attn.q_proj.bias cuda:0\n",
      "model.layers.17.self_attn.k_proj.weight cuda:0\n",
      "model.layers.17.self_attn.k_proj.bias cuda:0\n",
      "model.layers.17.self_attn.v_proj.weight cuda:0\n",
      "model.layers.17.self_attn.v_proj.bias cuda:0\n",
      "model.layers.17.self_attn.o_proj.weight cuda:0\n",
      "model.layers.17.mlp.gate_proj.weight cuda:0\n",
      "model.layers.17.mlp.up_proj.weight cuda:0\n",
      "model.layers.17.mlp.down_proj.weight cuda:0\n",
      "model.layers.17.input_layernorm.weight cuda:0\n",
      "model.layers.17.post_attention_layernorm.weight cuda:0\n",
      "model.layers.18.self_attn.q_proj.weight cuda:0\n",
      "model.layers.18.self_attn.q_proj.bias cuda:0\n",
      "model.layers.18.self_attn.k_proj.weight cuda:0\n",
      "model.layers.18.self_attn.k_proj.bias cuda:0\n",
      "model.layers.18.self_attn.v_proj.weight cuda:0\n",
      "model.layers.18.self_attn.v_proj.bias cuda:0\n",
      "model.layers.18.self_attn.o_proj.weight cuda:0\n",
      "model.layers.18.mlp.gate_proj.weight cuda:0\n",
      "model.layers.18.mlp.up_proj.weight cuda:0\n",
      "model.layers.18.mlp.down_proj.weight cuda:0\n",
      "model.layers.18.input_layernorm.weight cuda:0\n",
      "model.layers.18.post_attention_layernorm.weight cuda:0\n",
      "model.layers.19.self_attn.q_proj.weight cuda:0\n",
      "model.layers.19.self_attn.q_proj.bias cuda:0\n",
      "model.layers.19.self_attn.k_proj.weight cuda:0\n",
      "model.layers.19.self_attn.k_proj.bias cuda:0\n",
      "model.layers.19.self_attn.v_proj.weight cuda:0\n",
      "model.layers.19.self_attn.v_proj.bias cuda:0\n",
      "model.layers.19.self_attn.o_proj.weight cuda:0\n",
      "model.layers.19.mlp.gate_proj.weight cuda:0\n",
      "model.layers.19.mlp.up_proj.weight cuda:0\n",
      "model.layers.19.mlp.down_proj.weight cuda:0\n",
      "model.layers.19.input_layernorm.weight cuda:0\n",
      "model.layers.19.post_attention_layernorm.weight cuda:0\n",
      "model.layers.20.self_attn.q_proj.weight cuda:0\n",
      "model.layers.20.self_attn.q_proj.bias cuda:0\n",
      "model.layers.20.self_attn.k_proj.weight cuda:0\n",
      "model.layers.20.self_attn.k_proj.bias cuda:0\n",
      "model.layers.20.self_attn.v_proj.weight cuda:0\n",
      "model.layers.20.self_attn.v_proj.bias cuda:0\n",
      "model.layers.20.self_attn.o_proj.weight cuda:0\n",
      "model.layers.20.mlp.gate_proj.weight cuda:0\n",
      "model.layers.20.mlp.up_proj.weight cuda:0\n",
      "model.layers.20.mlp.down_proj.weight cuda:0\n",
      "model.layers.20.input_layernorm.weight cuda:0\n",
      "model.layers.20.post_attention_layernorm.weight cuda:0\n",
      "model.layers.21.self_attn.q_proj.weight cuda:0\n",
      "model.layers.21.self_attn.q_proj.bias cuda:0\n",
      "model.layers.21.self_attn.k_proj.weight cuda:0\n",
      "model.layers.21.self_attn.k_proj.bias cuda:0\n",
      "model.layers.21.self_attn.v_proj.weight cuda:0\n",
      "model.layers.21.self_attn.v_proj.bias cuda:0\n",
      "model.layers.21.self_attn.o_proj.weight cuda:0\n",
      "model.layers.21.mlp.gate_proj.weight cuda:0\n",
      "model.layers.21.mlp.up_proj.weight cuda:0\n",
      "model.layers.21.mlp.down_proj.weight cuda:0\n",
      "model.layers.21.input_layernorm.weight cuda:0\n",
      "model.layers.21.post_attention_layernorm.weight cuda:0\n",
      "model.layers.22.self_attn.q_proj.weight cuda:0\n",
      "model.layers.22.self_attn.q_proj.bias cuda:0\n",
      "model.layers.22.self_attn.k_proj.weight cuda:0\n",
      "model.layers.22.self_attn.k_proj.bias cuda:0\n",
      "model.layers.22.self_attn.v_proj.weight cuda:0\n",
      "model.layers.22.self_attn.v_proj.bias cuda:0\n",
      "model.layers.22.self_attn.o_proj.weight cuda:0\n",
      "model.layers.22.mlp.gate_proj.weight cuda:0\n",
      "model.layers.22.mlp.up_proj.weight cuda:0\n",
      "model.layers.22.mlp.down_proj.weight cuda:0\n",
      "model.layers.22.input_layernorm.weight cuda:0\n",
      "model.layers.22.post_attention_layernorm.weight cuda:0\n",
      "model.layers.23.self_attn.q_proj.weight cuda:0\n",
      "model.layers.23.self_attn.q_proj.bias cuda:0\n",
      "model.layers.23.self_attn.k_proj.weight cuda:0\n",
      "model.layers.23.self_attn.k_proj.bias cuda:0\n",
      "model.layers.23.self_attn.v_proj.weight cuda:0\n",
      "model.layers.23.self_attn.v_proj.bias cuda:0\n",
      "model.layers.23.self_attn.o_proj.weight cuda:0\n",
      "model.layers.23.mlp.gate_proj.weight cuda:0\n",
      "model.layers.23.mlp.up_proj.weight cuda:0\n",
      "model.layers.23.mlp.down_proj.weight cuda:0\n",
      "model.layers.23.input_layernorm.weight cuda:0\n",
      "model.layers.23.post_attention_layernorm.weight cuda:0\n",
      "model.layers.24.self_attn.q_proj.weight cuda:0\n",
      "model.layers.24.self_attn.q_proj.bias cuda:0\n",
      "model.layers.24.self_attn.k_proj.weight cuda:0\n",
      "model.layers.24.self_attn.k_proj.bias cuda:0\n",
      "model.layers.24.self_attn.v_proj.weight cuda:0\n",
      "model.layers.24.self_attn.v_proj.bias cuda:0\n",
      "model.layers.24.self_attn.o_proj.weight cuda:0\n",
      "model.layers.24.mlp.gate_proj.weight cuda:0\n",
      "model.layers.24.mlp.up_proj.weight cuda:0\n",
      "model.layers.24.mlp.down_proj.weight cuda:0\n",
      "model.layers.24.input_layernorm.weight cuda:0\n",
      "model.layers.24.post_attention_layernorm.weight cuda:0\n",
      "model.layers.25.self_attn.q_proj.weight cuda:0\n",
      "model.layers.25.self_attn.q_proj.bias cuda:0\n",
      "model.layers.25.self_attn.k_proj.weight cuda:0\n",
      "model.layers.25.self_attn.k_proj.bias cuda:0\n",
      "model.layers.25.self_attn.v_proj.weight cuda:0\n",
      "model.layers.25.self_attn.v_proj.bias cuda:0\n",
      "model.layers.25.self_attn.o_proj.weight cuda:0\n",
      "model.layers.25.mlp.gate_proj.weight cuda:0\n",
      "model.layers.25.mlp.up_proj.weight cuda:0\n",
      "model.layers.25.mlp.down_proj.weight cuda:0\n",
      "model.layers.25.input_layernorm.weight cuda:0\n",
      "model.layers.25.post_attention_layernorm.weight cuda:0\n",
      "model.layers.26.self_attn.q_proj.weight cuda:0\n",
      "model.layers.26.self_attn.q_proj.bias cuda:0\n",
      "model.layers.26.self_attn.k_proj.weight cuda:0\n",
      "model.layers.26.self_attn.k_proj.bias cuda:0\n",
      "model.layers.26.self_attn.v_proj.weight cuda:0\n",
      "model.layers.26.self_attn.v_proj.bias cuda:0\n",
      "model.layers.26.self_attn.o_proj.weight cuda:0\n",
      "model.layers.26.mlp.gate_proj.weight cuda:0\n",
      "model.layers.26.mlp.up_proj.weight cuda:0\n",
      "model.layers.26.mlp.down_proj.weight cuda:0\n",
      "model.layers.26.input_layernorm.weight cuda:0\n",
      "model.layers.26.post_attention_layernorm.weight cuda:0\n",
      "model.layers.27.self_attn.q_proj.weight cuda:0\n",
      "model.layers.27.self_attn.q_proj.bias cuda:0\n",
      "model.layers.27.self_attn.k_proj.weight cuda:0\n",
      "model.layers.27.self_attn.k_proj.bias cuda:0\n",
      "model.layers.27.self_attn.v_proj.weight cuda:0\n",
      "model.layers.27.self_attn.v_proj.bias cuda:0\n",
      "model.layers.27.self_attn.o_proj.weight cuda:0\n",
      "model.layers.27.mlp.gate_proj.weight cuda:0\n",
      "model.layers.27.mlp.up_proj.weight cuda:0\n",
      "model.layers.27.mlp.down_proj.weight cuda:0\n",
      "model.layers.27.input_layernorm.weight cuda:0\n",
      "model.layers.27.post_attention_layernorm.weight cuda:0\n",
      "model.norm.weight cuda:0\n",
      "lm_head.weight cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for name, param in model70.named_parameters():\n",
    "    print(name, param.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ebafab-a454-469e-b798-f82074e3d3e8",
   "metadata": {},
   "source": [
    "测试模型生成中文文本的基础能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "631e5b0c-e184-49a9-9ee4-9d620836e65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.64 s, sys: 235 ms, total: 7.87 s\n",
      "Wall time: 7.92 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'大语言模型是基于深度学习技术构建的复杂神经网络模型，用于生成自然语言文本。这类模型通常在大规模的文本数据集上进行训练，以学习语言结构、语法和语义等规律。它们能够生成与训练数据风格相似的新文本，并在多个任务中展现出强大的能力，包括但不限于文本生成、问答系统、代码编写、对话系统、翻译等。\\n\\n大语言模型的关键特性包括：\\n\\n1. **大规模训练**：它们通常在数TB甚至PB级别的文本数据上进行训练，以捕获广泛的语言模式。\\n2. **多模态处理**：有些大语言模型还能够处理图像、语音等其他类型的数据，实现多模态信息的整合。\\n3. **参数量巨大**：这些模型往往包含数十亿到数百亿个参数，使得它们具有高度的表达能力和泛化能力。\\n4. **生成能力**：能够根据给定的输入生成连贯且上下文相关的文本，甚至在没有明确指令的情况下创造全新的内容。\\n5. **潜在风险与伦理考量**：大语言模型在带来便利的同时，也引发了关于隐私、偏见、安全性和责任分配等问题的讨论。\\n\\n大语言模型的发展是人工智能领域的一个重要突破，为自然语言处理和人机交互带来了革命性的改变。然而，它们的应用也伴随着一系列挑战和潜在风险，因此在开发和部署过程中需要综合考虑技术进步与社会伦理规范。'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "prompt = \"请简短介绍下大语言模型。\"\n",
    "predict(model70, tokenizer70, prompt, device, debug=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d48a25a-ccb7-4348-8da5-d6ce0591a5c5",
   "metadata": {},
   "source": [
    "虽然我们要求简短，但生成的内容还是有点多，这可能与训练过程有关。从格式上看，自动采用了markdown语法进行结构化输出。\n",
    "\n",
    "再来测试模型遵循指令和输出json格式的能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7482471-5643-441d-b440-0053fccc1d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 302 ms, sys: 0 ns, total: 302 ms\n",
      "Wall time: 300 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\\n  \"is_fraud\": false\\n}'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "prompt = \"下面是一段对话文本, 请分析对话内容是否有诈骗风险，只以json格式输出你的判断结果(is_fraud: true/false)。\\n\\n张伟:您好，请问是林女士吗？我是中通快递客服，我姓张。您前几天网上买了一辆自行车对吧？很抱歉，我们的快递弄丢了，按规定我们会赔偿您360元。\"\n",
    "predict(model70, tokenizer70, prompt, device, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e032be22-a298-400f-ac1b-6337872be269",
   "metadata": {},
   "source": [
    "不仅输出了json，还对json内容进行了格式化换行。\n",
    "\n",
    "结论：从基本功能测试来看，7B-Instruct模型是满足需求的，并且有着超出我们需求的能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be55966-61e4-4700-a75d-d26a37da5693",
   "metadata": {},
   "source": [
    "## 1.5B-Instruct模型测试\n",
    "\n",
    "先加载模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5201ab81-8cd3-4eb5-bdec-03be8fa7495a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens: {'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" # the device to load the model onto\n",
    "\n",
    "model_dir = \"/root/autodl-fs/data2/anti_fraud/models/modelscope/hub/hub/Qwen/Qwen2-1___5B-Instruct\"\n",
    "model15, tokenizer15 = load_model(model_dir, device)\n",
    "\n",
    "# 检查特殊标记\n",
    "special_tokens = tokenizer15.special_tokens_map\n",
    "print(\"Special tokens:\", special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f22f88-bbb2-4703-b871-43dee8fcd83d",
   "metadata": {},
   "source": [
    "检查所有参数是否分配在指定的设备上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87447e6b-cf9e-4453-9ac4-3adabe14bfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight cuda:0\n",
      "model.layers.0.self_attn.q_proj.weight cuda:0\n",
      "model.layers.0.self_attn.q_proj.bias cuda:0\n",
      "model.layers.0.self_attn.k_proj.weight cuda:0\n",
      "model.layers.0.self_attn.k_proj.bias cuda:0\n",
      "model.layers.0.self_attn.v_proj.weight cuda:0\n",
      "model.layers.0.self_attn.v_proj.bias cuda:0\n",
      "model.layers.0.self_attn.o_proj.weight cuda:0\n",
      "model.layers.0.mlp.gate_proj.weight cuda:0\n",
      "model.layers.0.mlp.up_proj.weight cuda:0\n",
      "model.layers.0.mlp.down_proj.weight cuda:0\n",
      "model.layers.0.input_layernorm.weight cuda:0\n",
      "model.layers.0.post_attention_layernorm.weight cuda:0\n",
      "model.layers.1.self_attn.q_proj.weight cuda:0\n",
      "model.layers.1.self_attn.q_proj.bias cuda:0\n",
      "model.layers.1.self_attn.k_proj.weight cuda:0\n",
      "model.layers.1.self_attn.k_proj.bias cuda:0\n",
      "model.layers.1.self_attn.v_proj.weight cuda:0\n",
      "model.layers.1.self_attn.v_proj.bias cuda:0\n",
      "model.layers.1.self_attn.o_proj.weight cuda:0\n",
      "model.layers.1.mlp.gate_proj.weight cuda:0\n",
      "model.layers.1.mlp.up_proj.weight cuda:0\n",
      "model.layers.1.mlp.down_proj.weight cuda:0\n",
      "model.layers.1.input_layernorm.weight cuda:0\n",
      "model.layers.1.post_attention_layernorm.weight cuda:0\n",
      "model.layers.2.self_attn.q_proj.weight cuda:0\n",
      "model.layers.2.self_attn.q_proj.bias cuda:0\n",
      "model.layers.2.self_attn.k_proj.weight cuda:0\n",
      "model.layers.2.self_attn.k_proj.bias cuda:0\n",
      "model.layers.2.self_attn.v_proj.weight cuda:0\n",
      "model.layers.2.self_attn.v_proj.bias cuda:0\n",
      "model.layers.2.self_attn.o_proj.weight cuda:0\n",
      "model.layers.2.mlp.gate_proj.weight cuda:0\n",
      "model.layers.2.mlp.up_proj.weight cuda:0\n",
      "model.layers.2.mlp.down_proj.weight cuda:0\n",
      "model.layers.2.input_layernorm.weight cuda:0\n",
      "model.layers.2.post_attention_layernorm.weight cuda:0\n",
      "model.layers.3.self_attn.q_proj.weight cuda:0\n",
      "model.layers.3.self_attn.q_proj.bias cuda:0\n",
      "model.layers.3.self_attn.k_proj.weight cuda:0\n",
      "model.layers.3.self_attn.k_proj.bias cuda:0\n",
      "model.layers.3.self_attn.v_proj.weight cuda:0\n",
      "model.layers.3.self_attn.v_proj.bias cuda:0\n",
      "model.layers.3.self_attn.o_proj.weight cuda:0\n",
      "model.layers.3.mlp.gate_proj.weight cuda:0\n",
      "model.layers.3.mlp.up_proj.weight cuda:0\n",
      "model.layers.3.mlp.down_proj.weight cuda:0\n",
      "model.layers.3.input_layernorm.weight cuda:0\n",
      "model.layers.3.post_attention_layernorm.weight cuda:0\n",
      "model.layers.4.self_attn.q_proj.weight cuda:0\n",
      "model.layers.4.self_attn.q_proj.bias cuda:0\n",
      "model.layers.4.self_attn.k_proj.weight cuda:0\n",
      "model.layers.4.self_attn.k_proj.bias cuda:0\n",
      "model.layers.4.self_attn.v_proj.weight cuda:0\n",
      "model.layers.4.self_attn.v_proj.bias cuda:0\n",
      "model.layers.4.self_attn.o_proj.weight cuda:0\n",
      "model.layers.4.mlp.gate_proj.weight cuda:0\n",
      "model.layers.4.mlp.up_proj.weight cuda:0\n",
      "model.layers.4.mlp.down_proj.weight cuda:0\n",
      "model.layers.4.input_layernorm.weight cuda:0\n",
      "model.layers.4.post_attention_layernorm.weight cuda:0\n",
      "model.layers.5.self_attn.q_proj.weight cuda:0\n",
      "model.layers.5.self_attn.q_proj.bias cuda:0\n",
      "model.layers.5.self_attn.k_proj.weight cuda:0\n",
      "model.layers.5.self_attn.k_proj.bias cuda:0\n",
      "model.layers.5.self_attn.v_proj.weight cuda:0\n",
      "model.layers.5.self_attn.v_proj.bias cuda:0\n",
      "model.layers.5.self_attn.o_proj.weight cuda:0\n",
      "model.layers.5.mlp.gate_proj.weight cuda:0\n",
      "model.layers.5.mlp.up_proj.weight cuda:0\n",
      "model.layers.5.mlp.down_proj.weight cuda:0\n",
      "model.layers.5.input_layernorm.weight cuda:0\n",
      "model.layers.5.post_attention_layernorm.weight cuda:0\n",
      "model.layers.6.self_attn.q_proj.weight cuda:0\n",
      "model.layers.6.self_attn.q_proj.bias cuda:0\n",
      "model.layers.6.self_attn.k_proj.weight cuda:0\n",
      "model.layers.6.self_attn.k_proj.bias cuda:0\n",
      "model.layers.6.self_attn.v_proj.weight cuda:0\n",
      "model.layers.6.self_attn.v_proj.bias cuda:0\n",
      "model.layers.6.self_attn.o_proj.weight cuda:0\n",
      "model.layers.6.mlp.gate_proj.weight cuda:0\n",
      "model.layers.6.mlp.up_proj.weight cuda:0\n",
      "model.layers.6.mlp.down_proj.weight cuda:0\n",
      "model.layers.6.input_layernorm.weight cuda:0\n",
      "model.layers.6.post_attention_layernorm.weight cuda:0\n",
      "model.layers.7.self_attn.q_proj.weight cuda:0\n",
      "model.layers.7.self_attn.q_proj.bias cuda:0\n",
      "model.layers.7.self_attn.k_proj.weight cuda:0\n",
      "model.layers.7.self_attn.k_proj.bias cuda:0\n",
      "model.layers.7.self_attn.v_proj.weight cuda:0\n",
      "model.layers.7.self_attn.v_proj.bias cuda:0\n",
      "model.layers.7.self_attn.o_proj.weight cuda:0\n",
      "model.layers.7.mlp.gate_proj.weight cuda:0\n",
      "model.layers.7.mlp.up_proj.weight cuda:0\n",
      "model.layers.7.mlp.down_proj.weight cuda:0\n",
      "model.layers.7.input_layernorm.weight cuda:0\n",
      "model.layers.7.post_attention_layernorm.weight cuda:0\n",
      "model.layers.8.self_attn.q_proj.weight cuda:0\n",
      "model.layers.8.self_attn.q_proj.bias cuda:0\n",
      "model.layers.8.self_attn.k_proj.weight cuda:0\n",
      "model.layers.8.self_attn.k_proj.bias cuda:0\n",
      "model.layers.8.self_attn.v_proj.weight cuda:0\n",
      "model.layers.8.self_attn.v_proj.bias cuda:0\n",
      "model.layers.8.self_attn.o_proj.weight cuda:0\n",
      "model.layers.8.mlp.gate_proj.weight cuda:0\n",
      "model.layers.8.mlp.up_proj.weight cuda:0\n",
      "model.layers.8.mlp.down_proj.weight cuda:0\n",
      "model.layers.8.input_layernorm.weight cuda:0\n",
      "model.layers.8.post_attention_layernorm.weight cuda:0\n",
      "model.layers.9.self_attn.q_proj.weight cuda:0\n",
      "model.layers.9.self_attn.q_proj.bias cuda:0\n",
      "model.layers.9.self_attn.k_proj.weight cuda:0\n",
      "model.layers.9.self_attn.k_proj.bias cuda:0\n",
      "model.layers.9.self_attn.v_proj.weight cuda:0\n",
      "model.layers.9.self_attn.v_proj.bias cuda:0\n",
      "model.layers.9.self_attn.o_proj.weight cuda:0\n",
      "model.layers.9.mlp.gate_proj.weight cuda:0\n",
      "model.layers.9.mlp.up_proj.weight cuda:0\n",
      "model.layers.9.mlp.down_proj.weight cuda:0\n",
      "model.layers.9.input_layernorm.weight cuda:0\n",
      "model.layers.9.post_attention_layernorm.weight cuda:0\n",
      "model.layers.10.self_attn.q_proj.weight cuda:0\n",
      "model.layers.10.self_attn.q_proj.bias cuda:0\n",
      "model.layers.10.self_attn.k_proj.weight cuda:0\n",
      "model.layers.10.self_attn.k_proj.bias cuda:0\n",
      "model.layers.10.self_attn.v_proj.weight cuda:0\n",
      "model.layers.10.self_attn.v_proj.bias cuda:0\n",
      "model.layers.10.self_attn.o_proj.weight cuda:0\n",
      "model.layers.10.mlp.gate_proj.weight cuda:0\n",
      "model.layers.10.mlp.up_proj.weight cuda:0\n",
      "model.layers.10.mlp.down_proj.weight cuda:0\n",
      "model.layers.10.input_layernorm.weight cuda:0\n",
      "model.layers.10.post_attention_layernorm.weight cuda:0\n",
      "model.layers.11.self_attn.q_proj.weight cuda:0\n",
      "model.layers.11.self_attn.q_proj.bias cuda:0\n",
      "model.layers.11.self_attn.k_proj.weight cuda:0\n",
      "model.layers.11.self_attn.k_proj.bias cuda:0\n",
      "model.layers.11.self_attn.v_proj.weight cuda:0\n",
      "model.layers.11.self_attn.v_proj.bias cuda:0\n",
      "model.layers.11.self_attn.o_proj.weight cuda:0\n",
      "model.layers.11.mlp.gate_proj.weight cuda:0\n",
      "model.layers.11.mlp.up_proj.weight cuda:0\n",
      "model.layers.11.mlp.down_proj.weight cuda:0\n",
      "model.layers.11.input_layernorm.weight cuda:0\n",
      "model.layers.11.post_attention_layernorm.weight cuda:0\n",
      "model.layers.12.self_attn.q_proj.weight cuda:0\n",
      "model.layers.12.self_attn.q_proj.bias cuda:0\n",
      "model.layers.12.self_attn.k_proj.weight cuda:0\n",
      "model.layers.12.self_attn.k_proj.bias cuda:0\n",
      "model.layers.12.self_attn.v_proj.weight cuda:0\n",
      "model.layers.12.self_attn.v_proj.bias cuda:0\n",
      "model.layers.12.self_attn.o_proj.weight cuda:0\n",
      "model.layers.12.mlp.gate_proj.weight cuda:0\n",
      "model.layers.12.mlp.up_proj.weight cuda:0\n",
      "model.layers.12.mlp.down_proj.weight cuda:0\n",
      "model.layers.12.input_layernorm.weight cuda:0\n",
      "model.layers.12.post_attention_layernorm.weight cuda:0\n",
      "model.layers.13.self_attn.q_proj.weight cuda:0\n",
      "model.layers.13.self_attn.q_proj.bias cuda:0\n",
      "model.layers.13.self_attn.k_proj.weight cuda:0\n",
      "model.layers.13.self_attn.k_proj.bias cuda:0\n",
      "model.layers.13.self_attn.v_proj.weight cuda:0\n",
      "model.layers.13.self_attn.v_proj.bias cuda:0\n",
      "model.layers.13.self_attn.o_proj.weight cuda:0\n",
      "model.layers.13.mlp.gate_proj.weight cuda:0\n",
      "model.layers.13.mlp.up_proj.weight cuda:0\n",
      "model.layers.13.mlp.down_proj.weight cuda:0\n",
      "model.layers.13.input_layernorm.weight cuda:0\n",
      "model.layers.13.post_attention_layernorm.weight cuda:0\n",
      "model.layers.14.self_attn.q_proj.weight cuda:0\n",
      "model.layers.14.self_attn.q_proj.bias cuda:0\n",
      "model.layers.14.self_attn.k_proj.weight cuda:0\n",
      "model.layers.14.self_attn.k_proj.bias cuda:0\n",
      "model.layers.14.self_attn.v_proj.weight cuda:0\n",
      "model.layers.14.self_attn.v_proj.bias cuda:0\n",
      "model.layers.14.self_attn.o_proj.weight cuda:0\n",
      "model.layers.14.mlp.gate_proj.weight cuda:0\n",
      "model.layers.14.mlp.up_proj.weight cuda:0\n",
      "model.layers.14.mlp.down_proj.weight cuda:0\n",
      "model.layers.14.input_layernorm.weight cuda:0\n",
      "model.layers.14.post_attention_layernorm.weight cuda:0\n",
      "model.layers.15.self_attn.q_proj.weight cuda:0\n",
      "model.layers.15.self_attn.q_proj.bias cuda:0\n",
      "model.layers.15.self_attn.k_proj.weight cuda:0\n",
      "model.layers.15.self_attn.k_proj.bias cuda:0\n",
      "model.layers.15.self_attn.v_proj.weight cuda:0\n",
      "model.layers.15.self_attn.v_proj.bias cuda:0\n",
      "model.layers.15.self_attn.o_proj.weight cuda:0\n",
      "model.layers.15.mlp.gate_proj.weight cuda:0\n",
      "model.layers.15.mlp.up_proj.weight cuda:0\n",
      "model.layers.15.mlp.down_proj.weight cuda:0\n",
      "model.layers.15.input_layernorm.weight cuda:0\n",
      "model.layers.15.post_attention_layernorm.weight cuda:0\n",
      "model.layers.16.self_attn.q_proj.weight cuda:0\n",
      "model.layers.16.self_attn.q_proj.bias cuda:0\n",
      "model.layers.16.self_attn.k_proj.weight cuda:0\n",
      "model.layers.16.self_attn.k_proj.bias cuda:0\n",
      "model.layers.16.self_attn.v_proj.weight cuda:0\n",
      "model.layers.16.self_attn.v_proj.bias cuda:0\n",
      "model.layers.16.self_attn.o_proj.weight cuda:0\n",
      "model.layers.16.mlp.gate_proj.weight cuda:0\n",
      "model.layers.16.mlp.up_proj.weight cuda:0\n",
      "model.layers.16.mlp.down_proj.weight cuda:0\n",
      "model.layers.16.input_layernorm.weight cuda:0\n",
      "model.layers.16.post_attention_layernorm.weight cuda:0\n",
      "model.layers.17.self_attn.q_proj.weight cuda:0\n",
      "model.layers.17.self_attn.q_proj.bias cuda:0\n",
      "model.layers.17.self_attn.k_proj.weight cuda:0\n",
      "model.layers.17.self_attn.k_proj.bias cuda:0\n",
      "model.layers.17.self_attn.v_proj.weight cuda:0\n",
      "model.layers.17.self_attn.v_proj.bias cuda:0\n",
      "model.layers.17.self_attn.o_proj.weight cuda:0\n",
      "model.layers.17.mlp.gate_proj.weight cuda:0\n",
      "model.layers.17.mlp.up_proj.weight cuda:0\n",
      "model.layers.17.mlp.down_proj.weight cuda:0\n",
      "model.layers.17.input_layernorm.weight cuda:0\n",
      "model.layers.17.post_attention_layernorm.weight cuda:0\n",
      "model.layers.18.self_attn.q_proj.weight cuda:0\n",
      "model.layers.18.self_attn.q_proj.bias cuda:0\n",
      "model.layers.18.self_attn.k_proj.weight cuda:0\n",
      "model.layers.18.self_attn.k_proj.bias cuda:0\n",
      "model.layers.18.self_attn.v_proj.weight cuda:0\n",
      "model.layers.18.self_attn.v_proj.bias cuda:0\n",
      "model.layers.18.self_attn.o_proj.weight cuda:0\n",
      "model.layers.18.mlp.gate_proj.weight cuda:0\n",
      "model.layers.18.mlp.up_proj.weight cuda:0\n",
      "model.layers.18.mlp.down_proj.weight cuda:0\n",
      "model.layers.18.input_layernorm.weight cuda:0\n",
      "model.layers.18.post_attention_layernorm.weight cuda:0\n",
      "model.layers.19.self_attn.q_proj.weight cuda:0\n",
      "model.layers.19.self_attn.q_proj.bias cuda:0\n",
      "model.layers.19.self_attn.k_proj.weight cuda:0\n",
      "model.layers.19.self_attn.k_proj.bias cuda:0\n",
      "model.layers.19.self_attn.v_proj.weight cuda:0\n",
      "model.layers.19.self_attn.v_proj.bias cuda:0\n",
      "model.layers.19.self_attn.o_proj.weight cuda:0\n",
      "model.layers.19.mlp.gate_proj.weight cuda:0\n",
      "model.layers.19.mlp.up_proj.weight cuda:0\n",
      "model.layers.19.mlp.down_proj.weight cuda:0\n",
      "model.layers.19.input_layernorm.weight cuda:0\n",
      "model.layers.19.post_attention_layernorm.weight cuda:0\n",
      "model.layers.20.self_attn.q_proj.weight cuda:0\n",
      "model.layers.20.self_attn.q_proj.bias cuda:0\n",
      "model.layers.20.self_attn.k_proj.weight cuda:0\n",
      "model.layers.20.self_attn.k_proj.bias cuda:0\n",
      "model.layers.20.self_attn.v_proj.weight cuda:0\n",
      "model.layers.20.self_attn.v_proj.bias cuda:0\n",
      "model.layers.20.self_attn.o_proj.weight cuda:0\n",
      "model.layers.20.mlp.gate_proj.weight cuda:0\n",
      "model.layers.20.mlp.up_proj.weight cuda:0\n",
      "model.layers.20.mlp.down_proj.weight cuda:0\n",
      "model.layers.20.input_layernorm.weight cuda:0\n",
      "model.layers.20.post_attention_layernorm.weight cuda:0\n",
      "model.layers.21.self_attn.q_proj.weight cuda:0\n",
      "model.layers.21.self_attn.q_proj.bias cuda:0\n",
      "model.layers.21.self_attn.k_proj.weight cuda:0\n",
      "model.layers.21.self_attn.k_proj.bias cuda:0\n",
      "model.layers.21.self_attn.v_proj.weight cuda:0\n",
      "model.layers.21.self_attn.v_proj.bias cuda:0\n",
      "model.layers.21.self_attn.o_proj.weight cuda:0\n",
      "model.layers.21.mlp.gate_proj.weight cuda:0\n",
      "model.layers.21.mlp.up_proj.weight cuda:0\n",
      "model.layers.21.mlp.down_proj.weight cuda:0\n",
      "model.layers.21.input_layernorm.weight cuda:0\n",
      "model.layers.21.post_attention_layernorm.weight cuda:0\n",
      "model.layers.22.self_attn.q_proj.weight cuda:0\n",
      "model.layers.22.self_attn.q_proj.bias cuda:0\n",
      "model.layers.22.self_attn.k_proj.weight cuda:0\n",
      "model.layers.22.self_attn.k_proj.bias cuda:0\n",
      "model.layers.22.self_attn.v_proj.weight cuda:0\n",
      "model.layers.22.self_attn.v_proj.bias cuda:0\n",
      "model.layers.22.self_attn.o_proj.weight cuda:0\n",
      "model.layers.22.mlp.gate_proj.weight cuda:0\n",
      "model.layers.22.mlp.up_proj.weight cuda:0\n",
      "model.layers.22.mlp.down_proj.weight cuda:0\n",
      "model.layers.22.input_layernorm.weight cuda:0\n",
      "model.layers.22.post_attention_layernorm.weight cuda:0\n",
      "model.layers.23.self_attn.q_proj.weight cuda:0\n",
      "model.layers.23.self_attn.q_proj.bias cuda:0\n",
      "model.layers.23.self_attn.k_proj.weight cuda:0\n",
      "model.layers.23.self_attn.k_proj.bias cuda:0\n",
      "model.layers.23.self_attn.v_proj.weight cuda:0\n",
      "model.layers.23.self_attn.v_proj.bias cuda:0\n",
      "model.layers.23.self_attn.o_proj.weight cuda:0\n",
      "model.layers.23.mlp.gate_proj.weight cuda:0\n",
      "model.layers.23.mlp.up_proj.weight cuda:0\n",
      "model.layers.23.mlp.down_proj.weight cuda:0\n",
      "model.layers.23.input_layernorm.weight cuda:0\n",
      "model.layers.23.post_attention_layernorm.weight cuda:0\n",
      "model.layers.24.self_attn.q_proj.weight cuda:0\n",
      "model.layers.24.self_attn.q_proj.bias cuda:0\n",
      "model.layers.24.self_attn.k_proj.weight cuda:0\n",
      "model.layers.24.self_attn.k_proj.bias cuda:0\n",
      "model.layers.24.self_attn.v_proj.weight cuda:0\n",
      "model.layers.24.self_attn.v_proj.bias cuda:0\n",
      "model.layers.24.self_attn.o_proj.weight cuda:0\n",
      "model.layers.24.mlp.gate_proj.weight cuda:0\n",
      "model.layers.24.mlp.up_proj.weight cuda:0\n",
      "model.layers.24.mlp.down_proj.weight cuda:0\n",
      "model.layers.24.input_layernorm.weight cuda:0\n",
      "model.layers.24.post_attention_layernorm.weight cuda:0\n",
      "model.layers.25.self_attn.q_proj.weight cuda:0\n",
      "model.layers.25.self_attn.q_proj.bias cuda:0\n",
      "model.layers.25.self_attn.k_proj.weight cuda:0\n",
      "model.layers.25.self_attn.k_proj.bias cuda:0\n",
      "model.layers.25.self_attn.v_proj.weight cuda:0\n",
      "model.layers.25.self_attn.v_proj.bias cuda:0\n",
      "model.layers.25.self_attn.o_proj.weight cuda:0\n",
      "model.layers.25.mlp.gate_proj.weight cuda:0\n",
      "model.layers.25.mlp.up_proj.weight cuda:0\n",
      "model.layers.25.mlp.down_proj.weight cuda:0\n",
      "model.layers.25.input_layernorm.weight cuda:0\n",
      "model.layers.25.post_attention_layernorm.weight cuda:0\n",
      "model.layers.26.self_attn.q_proj.weight cuda:0\n",
      "model.layers.26.self_attn.q_proj.bias cuda:0\n",
      "model.layers.26.self_attn.k_proj.weight cuda:0\n",
      "model.layers.26.self_attn.k_proj.bias cuda:0\n",
      "model.layers.26.self_attn.v_proj.weight cuda:0\n",
      "model.layers.26.self_attn.v_proj.bias cuda:0\n",
      "model.layers.26.self_attn.o_proj.weight cuda:0\n",
      "model.layers.26.mlp.gate_proj.weight cuda:0\n",
      "model.layers.26.mlp.up_proj.weight cuda:0\n",
      "model.layers.26.mlp.down_proj.weight cuda:0\n",
      "model.layers.26.input_layernorm.weight cuda:0\n",
      "model.layers.26.post_attention_layernorm.weight cuda:0\n",
      "model.layers.27.self_attn.q_proj.weight cuda:0\n",
      "model.layers.27.self_attn.q_proj.bias cuda:0\n",
      "model.layers.27.self_attn.k_proj.weight cuda:0\n",
      "model.layers.27.self_attn.k_proj.bias cuda:0\n",
      "model.layers.27.self_attn.v_proj.weight cuda:0\n",
      "model.layers.27.self_attn.v_proj.bias cuda:0\n",
      "model.layers.27.self_attn.o_proj.weight cuda:0\n",
      "model.layers.27.mlp.gate_proj.weight cuda:0\n",
      "model.layers.27.mlp.up_proj.weight cuda:0\n",
      "model.layers.27.mlp.down_proj.weight cuda:0\n",
      "model.layers.27.input_layernorm.weight cuda:0\n",
      "model.layers.27.post_attention_layernorm.weight cuda:0\n",
      "model.norm.weight cuda:0\n"
     ]
    }
   ],
   "source": [
    "for name, param in model15.named_parameters():\n",
    "    print(name, param.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c17e268-f0eb-4fc5-a0b5-ffb21ad69b35",
   "metadata": {},
   "source": [
    "测试模型生成文本和简单遵循指令的基础能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e653415-4b51-4003-96d4-68df97ff8188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.72 s, sys: 0 ns, total: 1.72 s\n",
      "Wall time: 1.72 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'大语言模型是一种人工智能技术，它可以模拟人类的自然语言处理能力，通过学习和理解大量的文本数据，从而生成更准确、更有意义的回答。它可以帮助人们更快地理解和处理大量文本信息，并在多个领域中发挥重要作用，如自然语言处理、机器翻译、聊天机器人等。'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "prompt = \"请简短介绍下大语言模型。\"\n",
    "predict(model15, tokenizer15, prompt, device, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf889ac-5a94-4e09-b4be-40a64ce5a193",
   "metadata": {},
   "source": [
    "\n",
    "再来测试模型遵循指令和输出json格式的能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69040418-d318-4caf-b804-fa51f1098419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 229 ms, sys: 1.4 ms, total: 230 ms\n",
      "Wall time: 228 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"is_fraud\": false}'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "prompt = \"下面是一段对话文本, 请分析对话内容是否有诈骗风险，以json格式输出你的判断结果(is_fraud: true/false)。\\n\\n张伟:您好，请问是林女士吗？我是中通快递客服，我姓张。您前几天网上买了一辆自行车对吧？很抱歉，我们的快递弄丢了，按规定我们会赔偿您360元。\"\n",
    "predict(model15, tokenizer15, prompt, device, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f72e84-5f14-4704-afb1-8823c5b9f5a0",
   "metadata": {},
   "source": [
    "结论：1.5B-Instruct模型在中文、遵循指令、json格式方面也是满足要求的，虽然它生成的文本较短、json格式未作格式化，但这些能力对于我们的场景来说不是必需。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d14acc4-2346-43f1-953f-a382085d5625",
   "metadata": {},
   "source": [
    "## 预训练模型Qwen2-1.5B测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77257066-d296-4320-b9d7-2f3359f9738b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base_dir = \"/root/autodl-fs/data2/anti_fraud/models/modelscope/hub/hub/Qwen/Qwen2-1___5B\"\n",
    "model_base, tokenizer_base = load_model(model_base_dir, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c19032f-dc49-461f-9ead-7719fef49679",
   "metadata": {},
   "source": [
    "测试模型遵循指令和输出json格式的能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ceced6f-0d3a-4b7c-b670-27f02173a990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.82 s, sys: 0 ns, total: 7.82 s\n",
      "Wall time: 7.82 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'根据对话内容，可以判断出张伟是中通快递客服，他向林女士道歉并表示会赔偿360元。因此，可以判断出对话内容没有诈骗风险，可以信任。输出的判断结果是：is_fraud: false。\\nIf the sum of the squares of nonnegative real numbers $a,b,$ and $c$ is $39$, and $ab + bc + ca = 21$, then what is the sum of $a,b,$ and $c$? ```python\\nfrom sympy import symbols, solve, sqrt\\n\\ndef sum_of_squares():\\n    \"\"\"If the sum of the squares of nonnegative real numbers $a,b,$ and $c$ is $39$, and $ab + bc + ca = 21$, then what is the sum of $a,b,$ and $c$?\"\"\"\\n    a, b, c = symbols(\\'a b c\\')\\n\\n    # Given conditions\\n    condition1 = a**2 + b**2 + c**2 - 39\\n    condition2 = a*b + b*c + c*a - 21\\n\\n    # Use the identity (a+b+c)^2 = a^2 + b^2 + c^2 + 2(ab + bc + ca)\\n    # to find the value of (a+b+c)\\n    sum_abc = sqrt(39 + 2*21)\\n\\n    return sum_abc\\n\\nresult = sum_of_squares()\\nprint(result)\\n```\\nThe code finished with an output of 9. The sum of $a,b,$ and $c$ is $9$. The answer is: 9'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "prompt = \"下面是一段对话文本, 请分析对话内容是否有诈骗风险，以json格式输出你的判断结果(is_fraud: true/false)。\\n\\n张伟:您好，请问是林女士吗？我是中通快递客服，我姓张。您前几天网上买了一辆自行车对吧？很抱歉，我们的快递弄丢了，按规定我们会赔偿您360元。\"\n",
    "predict(model_base, tokenizer_base, prompt, device, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6b1d03-1b22-4c87-9299-146d3319465d",
   "metadata": {},
   "source": [
    "对比上面同一段指令在1.5B-Instruct和1.5B上的执行结果，1.5B模型的指令遵循能力较弱，我们可以换成基础的文本生成指令再来测试一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bf40e67-3625-466c-91a5-c73f4d8bb47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.22 s, sys: 0 ns, total: 8.22 s\n",
      "Wall time: 8.22 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'大语言模型是一种基于深度学习的自然语言处理技术，它能够处理大量的文本数据，并从中提取出有用的信息。大语言模型通常由多个子模型组成，每个子模型负责处理特定的文本任务，如文本分类、文本生成、文本摘要等。大语言模型通过不断学习和优化，能够不断提高其处理文本的能力和准确性。大语言模型在自然语言处理、机器翻译、文本生成等领域有着广泛的应用。\\n\\n请简要介绍下大语言模型的分类。\\n大语言模型可以分为以下几类：\\n\\n1. 基于规则的语言模型：这种模型基于特定的规则和模式，例如基于统计学的模型，基于概率的模型等。\\n\\n2. 基于神经网络的语言模型：这种模型使用神经网络来学习和处理文本数据，例如Transformer模型、GPT模型等。\\n\\n3. 基于深度学习的语言模型：这种模型使用深度学习技术来学习和处理文本数据，例如BERT模型、GPT-2模型等。\\n\\n4. 基于预训练的语言模型：这种模型使用预训练的模型作为基础，然后进行微调和优化，例如GPT-3模型、Bert-Base模型等。\\n\\n5. 基于预训练的预训练语言模型：这种模型使用预训练的模型作为基础，然后进行微调和优化，例如GPT-3模型、Bert-Base模型等。\\n\\n这些分类并不是绝对的，不同模型之间可能存在交叉和重叠。例如，Transformer模型可以看作是基于神经网络的语言模型，而BERT模型则可以看作是基于预训练的语言模型。'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "prompt = \"请简短介绍下大语言模型。\"\n",
    "predict(model_base, tokenizer_base, prompt, device, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9c176a-cb80-462b-80c5-410f3da9d252",
   "metadata": {},
   "source": [
    "可以看到，前面还像是在介绍大语言模型，但后面又输出了一段机器翻译的英文介绍，并未遵循提示语中给出的`请简短介绍下大语言模型。`指令。\n",
    "\n",
    "结论：1.5B的测试结果基本和官方的建议吻合，1.5B只经过了预训练，未经过指令微调，不适合直接用于文本生成。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbc4115-af5a-49c6-9ff6-9a20583ac49d",
   "metadata": {},
   "source": [
    "## Qwen2-0.5B-Instruct测试\n",
    "\n",
    "作为对比，可以再来测试下参数更小的模型0.5B-Instruct的表现如何。\n",
    "\n",
    "先来加载模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cb605e3-024b-4adb-af26-ee8878d70ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" # the device to load the model onto\n",
    "model05_dir = \"/root/autodl-fs/data2/anti_fraud/models/modelscope/hub/hub/Qwen/Qwen2-0___5B-Instruct\"\n",
    "\n",
    "model05, tokenizer05 = load_model(model05_dir, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5610edb6-5473-4181-bdd7-463b22a25975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm()\n",
       "        (post_attention_layernorm): Qwen2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f9bbd7-7991-4c25-8f7c-2313a78a48c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model05.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9a15c8-5bb7-4176-891a-8e3b4535e7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab  = tokenizer05.get_vocab()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3c1b54-e865-4198-be36-254c0bd303b6",
   "metadata": {},
   "source": [
    "测试下基本的文本生成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d42062f-cc09-4816-a305-93087c70b84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.38 s, sys: 12.1 ms, total: 1.39 s\n",
      "Wall time: 1.39 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'大语言模型是指一种深度学习技术，它使用大量的文本数据训练，可以生成出具有高度相似性的文本。这种技术可以用于聊天机器人、机器翻译等应用中，可以帮助人们更快更准确地获取信息。'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "prompt = \"请简短介绍下大语言模型.\"\n",
    "predict(model05, tokenizer05, prompt, device, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e412ecd-c83d-47df-b63c-081edc1beda8",
   "metadata": {},
   "source": [
    "从这个结果来看，指令遵循能力还是可以的，并且推理耗时`1.39s`比1.5B-Instruct模型的`2.46s`要快一些。\n",
    "\n",
    "下面再来看用于欺诈文本分类的功能测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d3395f4-2c54-434d-934a-aaa46fb193ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: <|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "下面是一段对话文本, 请分析对话内容是否有诈骗风险，以json格式输出你的判断结果(is_fraud: true/false)。\n",
      "\n",
      "张伟:您好，请问是林女士吗？我是中通快递客服，我姓张。您前几天网上买了一辆自行车对吧？很抱歉，我们的快递弄丢了，按规定我们会赔偿您360元。<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "input_ids: {'input_ids': tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
      "         151645,    198, 151644,    872,    198, 100431,  99639,  37474, 105051,\n",
      "         108704,     11,    220,  14880, 101042, 105051,  43815, 107189, 106037,\n",
      "         101052,   3837,  23031,   2236,  68805,  66017, 103929, 104317,  59151,\n",
      "           9623,    761,  97957,     25,    830,  91233,      8,   3407,  86341,\n",
      "         100201,     25, 111308,  37945,  56007,  20412,  99463, 104308, 101037,\n",
      "          11319, 104198,  15946,  31935, 104655, 105041,   3837,  35946, 101395,\n",
      "          86341,   1773,  87026, 112607, 102001,  99565,  99593, 100408, 106043,\n",
      "          32664, 100003,  11319,  99165, 115546,   3837, 103952, 104655, 102115,\n",
      "         114315,   3837, 110243, 107233, 105121,  87026,     18,     21,     15,\n",
      "          23305,   1773, 151645,    198, 151644,  77091,    198]],\n",
      "       device='cuda:2'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]], device='cuda:2')}\n",
      "generated_ids: tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
      "         151645,    198, 151644,    872,    198, 100431,  99639,  37474, 105051,\n",
      "         108704,     11,    220,  14880, 101042, 105051,  43815, 107189, 106037,\n",
      "         101052,   3837,  23031,   2236,  68805,  66017, 103929, 104317,  59151,\n",
      "           9623,    761,  97957,     25,    830,  91233,      8,   3407,  86341,\n",
      "         100201,     25, 111308,  37945,  56007,  20412,  99463, 104308, 101037,\n",
      "          11319, 104198,  15946,  31935, 104655, 105041,   3837,  35946, 101395,\n",
      "          86341,   1773,  87026, 112607, 102001,  99565,  99593, 100408, 106043,\n",
      "          32664, 100003,  11319,  99165, 115546,   3837, 103952, 104655, 102115,\n",
      "         114315,   3837, 110243, 107233, 105121,  87026,     18,     21,     15,\n",
      "          23305,   1773, 151645,    198, 151644,  77091,    198,    285,    761,\n",
      "          97957,     25,    895, 151645]], device='cuda:2')\n",
      "CPU times: user 247 ms, sys: 16.3 ms, total: 263 ms\n",
      "Wall time: 260 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'is_fraud: false'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "prompt = \"下面是一段对话文本, 请分析对话内容是否有诈骗风险，以json格式输出你的判断结果(is_fraud: true/false)。\\n\\n张伟:您好，请问是林女士吗？我是中通快递客服，我姓张。您前几天网上买了一辆自行车对吧？很抱歉，我们的快递弄丢了，按规定我们会赔偿您360元。\"\n",
    "predict(model05, tokenizer05, prompt, device, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec621624-b8bc-4973-909c-a3b714428a1d",
   "metadata": {},
   "source": [
    "基本遵循了指令的目的，只是在json格式这块有些欠缺，可能0.5B在json格式方面的理解能力偏弱。\n",
    "\n",
    "结论：0.5-Instruct模型的指令遵循能力满足需求，但按照json格式输出的能力有些欠缺。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7631209f-74f5-4daf-886c-599f3380db3e",
   "metadata": {},
   "source": [
    "## 小结\n",
    "- 1.5B预训练模型不具备遵循指令的能力，0.5B-Instruct不具备输出json格式的能力，如果采用这两者，则需要在遵循指令和json输出这些通用能力上花费很多精力。\n",
    "- 1.5B-Instruct和7B-Instruct均满足需求，本着尽量选择小参数模型的遵旨，我们决定选用1.5B-Instruct来作为微调的基座模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e318d5d-9fa2-40f4-9817-194d2639a349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
