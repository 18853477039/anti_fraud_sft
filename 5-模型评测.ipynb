{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "618092e3-00e1-4cb2-89fb-7e58edd3c2f6",
   "metadata": {},
   "source": [
    "## 引言\n",
    "前面一篇文章[欺诈文本分类微调（四）：构造训练/测试数据集](https://golfxiao.blog.csdn.net/article/details/141325192)已经构造出了测试数据集，这篇文章将基于测试数据集做模型评测，由于还没有开始训练，所以先对基座模型做评测。\n",
    "\n",
    "我们的任务目标是对文本进行分类，所以评测的目标是计算模型的精确率和召回率。评测的过程大概是：\n",
    "1. 用目标模型对每条数据的input做推理，得到一个预测值。\n",
    "2. 收集所有数据的预测值和标签值，并比较预测正确和预测错误的数量。\n",
    "3. 根据比较结果计算模型的精确率和召回率。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfe90d1-2c85-4c2d-a434-aed796816367",
   "metadata": {},
   "source": [
    "## 准备数据和模型\n",
    "\n",
    "先导入需要用到的库，其中：\n",
    "- transformers用于加载原始模型\n",
    "- peft用于加载微调模型\n",
    "- sklearn.metrics用于计算精确率和召回率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "094b7106-96e6-4114-b69c-4425e795f915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "from tqdm import *\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5af9952-d0ec-4c50-8732-0c4ce7bc2acc",
   "metadata": {},
   "source": [
    "定义评估数据集所在文件路径，以及原始模型路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53dce413-bd28-450f-82d8-1d4d9a2ea35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata_path = '/data2/anti_fraud/dataset/eval0819.jsonl'\n",
    "model_path = '/data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct'\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea705f3b-e606-4224-b375-4dd79205aac1",
   "metadata": {},
   "source": [
    "#### 加载数据\n",
    "\n",
    "上面定义的评估数据集采用jsonl格式保存，所以需要一个方法来加载jsonl格式的数据集，本质上就是用json.loads分别加载每条数据，最后再组成一个list。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99e17f52-af13-432c-a746-4c3aaf905015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(path):\n",
    "    with open(path, 'r') as file:\n",
    "        data = [json.loads(line) for line in file]\n",
    "        return data\n",
    "\n",
    "test_data = load_jsonl(testdata_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ceff2c-801d-4337-82d5-32ed2e2b9f13",
   "metadata": {},
   "source": [
    "查看下数据集是否均衡。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ec311a3-bd49-4c84-b04d-69b18e1f5e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_count: 2348, true_data: 1183, false_data: 1165\n"
     ]
    }
   ],
   "source": [
    "true_data = [d for d in test_data if d['label'] == True]\n",
    "false_data = [d for d in test_data if d['label'] == False]\n",
    "print(f\"total_count: {len(test_data)}, true_data: {len(true_data)}, false_data: {len(false_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc21311e-0aee-448e-ad4b-e74e6f5578f6",
   "metadata": {},
   "source": [
    "#### 加载模型\n",
    "定义一个方法load_model来同时支持加载原始模型和微调后的模型，使用时的区别在于是否传参微调后的checkpoint_path。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74a6d5b0-b845-4bf0-8805-344bf5f433b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, checkpoint_path='', device='cuda'):\n",
    "    # 加载tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, padding_side=\"left\")\n",
    "    # 加载模型\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, trust_remote_code=True).eval().to(device)\n",
    "    # 加载lora权重\n",
    "    if checkpoint_path: \n",
    "        model = PeftModel.from_pretrained(model, model_id=checkpoint_path).to(device)\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50359c1d-5b0d-4b3a-a1f8-36e096bc8da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 1536)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
       "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
       "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, tokenizer = load_model(model_path)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95522ab8-1cd7-4291-bac9-5932510a4094",
   "metadata": {},
   "source": [
    "定义一个推理函数predict，用传入的模型和一条对话文本来进行欺诈文本分类的推理。此函数定义时要考虑以下几点：\n",
    "1. 既要测试原始模型，也要测试微调后的模型，并且可能会微调多个版本，所以把model和tokenizer作为参数传入。\n",
    "2. 由于模型预测结果的不确定性，在使用json加载解析response时可能会报异常，需要加一个safe_loads保护。\n",
    "\n",
    "> 注：语言模型在生成json时，很容易输出markdown文本中嵌入的json格式，所以在解析json之前先尝试去除markdown代码块的头和尾。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39ce2e34-771a-4e16-990b-08ea45d4a11d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_fraud': False}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def safe_loads(text, default_value=None):\n",
    "    json_string = re.sub(r'^```json\\n(.*)\\n```$', r'\\1', text.strip(), flags=re.DOTALL)\n",
    "    try:  \n",
    "        return json.loads(json_string)  \n",
    "    except json.JSONDecodeError as e:  \n",
    "        print(f\"invalid json: {json_string}\")\n",
    "        return default_value  \n",
    "\n",
    "def predict(model, tokenizer, content, device='cuda', debug=False):\n",
    "    prompt = f\"下面是一段对话文本, 请分析对话内容是否有诈骗风险，只以json格式输出你的判断结果(is_fraud: true/false)。\\n\\n{content}\"\n",
    "    inputs = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}],\n",
    "                                           add_generation_prompt=True,\n",
    "                                           tokenize=True,\n",
    "                                           return_tensors=\"pt\",\n",
    "                                           return_dict=True\n",
    "                                           ).to(device)\n",
    "    \n",
    "    print(f\"inputs.shape: {inputs['input_ids'].shape[1]}\") if debug else None\n",
    "    default_response = {'is_fraud':False}\n",
    "    gen_kwargs = {\"max_new_tokens\": 2048, \"do_sample\": True, \"top_k\": 1}\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, **gen_kwargs)\n",
    "        print(f\"outputs.shape: {outputs.shape}\") if debug else None\n",
    "        outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return safe_loads(response, default_response)\n",
    "\n",
    "predict(model, tokenizer, '郎艳: 最近找到了一条新的货源，在重庆那边，如果您愿意投资，可以以很低的价格买到衣服，并且出租赚钱哦。投资金额500元到10000元不等，非常划算的。')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ada37f4-34a6-4fc7-9eff-391639cc326d",
   "metadata": {},
   "source": [
    "#### 评测方法\n",
    "\n",
    "评测主要是对所有数据的预测结果进行是否正确的统计与分析，最终根据自己的侧重点来计算出一两个指标用于评估模型性能。\n",
    "\n",
    "首先定义一个方法，对所有的test_data来进行分类预测，并返回所有数据的真实标签和预测标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b518db3b-e8c8-4a78-a9ed-24e10ecc266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(model, tokenizer, test_data, device='cuda', debug=False):\n",
    "    real_labels = []\n",
    "    pred_labels = []\n",
    "    pbar = tqdm(total=len(test_data), desc=f'progress')\n",
    "    \n",
    "    for i, item in enumerate(test_data):\n",
    "        dialog_input = item['input']\n",
    "        real_label = item['label']\n",
    "        \n",
    "        prediction = predict(model, tokenizer, dialog_input, device)\n",
    "        pred_label = prediction['is_fraud']\n",
    "        \n",
    "        real_labels.append(real_label)\n",
    "        pred_labels.append(pred_label)\n",
    "        \n",
    "        pbar.update()\n",
    "        # print(f\"percent: {(i*100)/len(test_data):.2f}%\") if (debug and i%(len(test_data)//20 + 1)==0) else None\n",
    "    return real_labels, pred_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090af225-d553-4c35-8647-61cffee0b76d",
   "metadata": {},
   "source": [
    "用一个迷你数据集（只有10条数据）来测试一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f05ba61b-3e17-4889-a339-312697846977",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress: 100%|██████████| 10/10 [00:01<00:00,  6.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([False, False, False, True, True, False, True, True, False, False],\n",
       " [True, False, False, False, False, False, False, True, False, False])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_labels, pred_labels = run_test(model, tokenizer, test_data[:10], debug=True)\n",
    "real_labels, pred_labels"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d3d446fc-cce4-43a0-9ae3-712006fa8471",
   "metadata": {},
   "source": [
    "对于二分类问题，常用的评测指标是计算精确率和召回率。\n",
    "\n",
    "定义计算召回率和精确率的方法，相关方法和概念解释如下：\n",
    "- confusion_matrix函数接受一个标签分类值集合和预测分类值集合，返回一个2x2的矩阵包含4个数据，分别是真负、假正、假负、真正的统计数字。\n",
    "- precision:精确率，表示预测为正的结果中有多少是真的正值，计算公式为：精确率=真正/(真正+假正）\n",
    "- recall：召回率，表示标签中为正的数据有多少被成功预测召回，计算公式为：召回率=真正/(真正+假负）                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a1cfa03-6620-4a8b-ac2e-3c329a292bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn：1, fp:3, fn:1, tp:5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.625, 0.8333333333333334)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def precision_recall(true_labels, pred_labels, labels=None, debug=False):\n",
    "    cm = confusion_matrix(true_labels, pred_labels, labels=labels)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(f\"tn：{tn}, fp:{fp}, fn:{fn}, tp:{tp}\") if debug else None\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    return precision, recall\n",
    "\n",
    "precision_recall(real_labels, pred_labels, labels=[True, False], debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797ead04-91bf-4987-900c-e43b087087c3",
   "metadata": {},
   "source": [
    "> 上面的(0.625, 0.833)就是模型在上面10条数据上计算出的精确率和召回率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a8c274-bc10-490b-be36-0004318525f8",
   "metadata": {},
   "source": [
    "## 运行评测\n",
    "\n",
    "将上面的步骤封装到一个evaluate方法中，这样只需要一句代码就能运行评估并输出结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5f1714e-cda0-4bb5-abfe-7be6942c5222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_path, checkpoint_path, dataset, device='cuda', debug=False):\n",
    "    model, tokenizer = load_model(model_path, checkpoint_path, device)\n",
    "    true_labels, pred_labels = run_test(model, tokenizer, dataset, device, debug=debug)\n",
    "    precision, recall = precision_recall(true_labels, pred_labels, debug=debug)\n",
    "    print(f\"precision: {precision}, recall: {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646f67a5-350f-4a90-b524-7ce9771cb11c",
   "metadata": {},
   "source": [
    "计算原始模型的精确率和召回率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8537987e-24f5-4ea7-95d1-75707bd6900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a900e70f-85c9-48a5-9e83-1fc810594db2",
   "metadata": {},
   "source": [
    "#### 原始模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f890ff2a-117b-4766-b1f1-b20c55d6bff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress:   8%|▊         | 192/2348 [00:28<10:16,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid json: 很抱歉，由于我无法直接查看或访问任何外部数据集，因此无法分析对话内容是否包含欺诈行为。我的功能仅限于回答一般性问题和提供信息。如果您有其他相关的问题，请随时提问。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress:  79%|███████▉  | 1855/2348 [04:26<01:12,  6.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid json: [{'is_fraud': True}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress:  80%|███████▉  | 1875/2348 [04:29<02:16,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid json: 很抱歉，由于我无法直接查看或访问任何外部数据集，因此无法分析对话内容是否包含欺诈行为。我的功能仅限于回答一般性问题和提供信息。如果您有其他相关的问题，请随时提问。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress: 100%|██████████| 2348/2348 [05:36<00:00,  6.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn：1091, fp:74, fn:638, tp:545\n",
      "precision: 0.8804523424878837, recall: 0.4606931530008453\n"
     ]
    }
   ],
   "source": [
    "evaluate(model_path, '', test_data, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8960e6b8-09e6-4f4c-a29c-f45ceabb1f2d",
   "metadata": {},
   "source": [
    "## 封装脚本\n",
    "由于模型微调往往要反复调参进行训练，每次训练完都需要用上面的过程来评测模型的精确率和召回率指标，所以有必要将上面的评测过程封装为一个`evaluate.py`，以方便使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef390f27-c6a1-4ec5-ab83-ed6335fcea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "from typing import List, Dict\n",
    "from tqdm import *\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, auc\n",
    "\n",
    "def load_jsonl(path):\n",
    "    with open(path, 'r') as file:\n",
    "        data = [json.loads(line) for line in file]\n",
    "        return data\n",
    "\n",
    "def load_model(model_path, checkpoint_path='', device='cuda'):\n",
    "    # 加载tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    # 加载模型\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, trust_remote_code=True).eval().to(device)\n",
    "    # 加载lora权重\n",
    "    if checkpoint_path: \n",
    "        model = PeftModel.from_pretrained(model, model_id=checkpoint_path).to(device)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def safe_loads(text, default_value=None):\n",
    "    json_string = re.sub(r'^```json\\n(.*)\\n```$', r'\\1', text.strip(), flags=re.DOTALL)\n",
    "    try:  \n",
    "        return json.loads(json_string)  \n",
    "    except json.JSONDecodeError as e:  \n",
    "        print(f\"invalid json: {json_string}\")\n",
    "        return default_value  \n",
    "\n",
    "def predict(model, tokenizer, content, device='cuda', debug=False):\n",
    "    prompt = f\"下面是一段对话文本, 请分析对话内容是否有诈骗风险，只以json格式输出你的判断结果(is_fraud: true/false)。\\n\\n{content}\"\n",
    "    inputs = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}],\n",
    "                                           add_generation_prompt=True,\n",
    "                                           tokenize=True,\n",
    "                                           return_tensors=\"pt\",\n",
    "                                           return_dict=True\n",
    "                                           ).to(device)\n",
    "    \n",
    "    default_response = {'is_fraud':False}\n",
    "    gen_kwargs = {\"max_new_tokens\": 2048, \"do_sample\": True, \"top_k\": 1}\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, **gen_kwargs)\n",
    "        outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return safe_loads(response, default_response)\n",
    "\n",
    "def run_test(model, tokenizer, test_data, device='cuda', debug=False):\n",
    "    real_labels = []\n",
    "    pred_labels = []\n",
    "    pbar = tqdm(total=len(test_data), desc=f'progress')\n",
    "    for i, item in enumerate(test_data):\n",
    "        dialog_input = item['input']\n",
    "        real_label = item['label']\n",
    "        \n",
    "        prediction = predict(model, tokenizer, dialog_input, device)\n",
    "        pred_label = prediction['is_fraud']\n",
    "        \n",
    "        real_labels.append(real_label)\n",
    "        pred_labels.append(pred_label)\n",
    "        pbar.update(1)\n",
    "    return real_labels, pred_labels\n",
    "\n",
    "def precision_recall(true_labels, pred_labels, labels=None, debug=False):\n",
    "    cm = confusion_matrix(true_labels, pred_labels, labels=labels)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(f\"tn：{tn}, fp:{fp}, fn:{fn}, tp:{tp}\") if debug else None\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    return precision, recall\n",
    "\n",
    "def evaluate_with_model(model, tokenizer, testdata_path, device='cuda', debug=False):\n",
    "    dataset = load_jsonl(testdata_path)\n",
    "    run_test_func = run_test_batch if batch else run_test\n",
    "    true_labels, pred_labels = run_test_func(model, tokenizer, dataset, device=device, debug=debug)\n",
    "    precision, recall = precision_recall(true_labels, pred_labels, debug=debug)\n",
    "    print(f\"precision: {precision}, recall: {recall}\")\n",
    "\n",
    "def evaluate(model_path, checkpoint_path, testdata_path, device='cuda', debug=False):    \n",
    "    model, tokenizer = load_model(model_path, checkpoint_path, device)\n",
    "    evaluate_with_model(model, tokenizer, testdata_path, device, batch, debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f5b6b9-982e-47ea-bf4f-c22b237bc3cc",
   "metadata": {},
   "source": [
    "#### 7B模型评测对比\n",
    "\n",
    "使用此脚本对Qwen2-7B模型的评测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e222713-3907-44b9-9468-4f2d17274fc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d42e55486f2943e59b77ac5c34c60837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress:   5%|▍         | 112/2348 [00:10<05:54,  6.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid json: 为了提供准确的分析，我需要完整的对话上下文。仅凭“发言人3: 明白”这一句话无法判断是否存在诈骗风险。通常在识别潜在诈骗时，会关注对话中的具体细节、请求或信息，比如涉及金钱交易、紧急情况、个人信息索取等。\n",
      "\n",
      "因此，基于提供的信息：\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"is_fraud\": false\n",
      "}\n",
      "```\n",
      "\n",
      "请注意，这只是一个基于当前信息的假设性判断。在实际情况下，可能需要更多上下文来做出准确判断。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress:   5%|▌         | 128/2348 [00:11<04:38,  7.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid json: 为了提供准确的分析结果，我需要看到具体的对话内容。请提供对话文本，然后我可以帮助您分析是否存在诈骗风险。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress:   8%|▊         | 184/2348 [00:16<04:04,  8.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid json: 为了提供准确的判断，我需要完整的对话上下文。仅凭“发言人1: 可以可以。”这一句话无法确定是否存在诈骗风险。通常，诈骗可能涉及虚假承诺、紧急情况要求转账、个人信息索取等。请提供完整对话内容以便进行分析。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress:   8%|▊         | 192/2348 [00:17<04:07,  8.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid json: 为了提供准确的分析结果，我需要看到具体的对话内容。请提供对话文本，然后我可以帮助您分析是否存在诈骗风险。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress:  10%|▉         | 224/2348 [00:22<06:33,  5.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid json: 为了准确地判断这段对话是否存在诈骗风险，我们需要更多的上下文信息来理解发言人的意图和对话的背景。仅凭“发言人1: 安全”这一句话，无法确定是否存在诈骗行为，因为这句话本身并不包含任何明显的欺诈信号或请求。\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"is_fraud\": false\n",
      "}\n",
      "```\n",
      "\n",
      "请注意，如果在实际场景中，这句话是在一个可能涉及敏感信息交换或交易的对话中出现的，那么需要更多的上下文信息来进行更准确的判断。在没有额外信息的情况下，基于提供的单句对话，我的判断是不存在诈骗风险。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress:  59%|█████▉    | 1392/2348 [01:48<02:28,  6.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid json: 为了提供准确的分析，我需要完整的对话内容。然而，根据您提供的信息，对话只包含“发言人2: 就是”这一句话，没有足够的上下文来判断是否存在诈骗风险。因此，基于这句信息，我无法确定是否涉及诈骗。\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"is_fraud\": null\n",
      "}\n",
      "```\n",
      "\n",
      "请注意，通常在实际应用中，会使用机器学习模型和大量的训练数据来分析对话中的关键词、语气、上下文等多方面因素，以更准确地判断是否存在诈骗风险。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress:  64%|██████▍   | 1504/2348 [01:59<02:22,  5.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid json: 为了准确地评估对话中的潜在诈骗风险，我需要更多的上下文信息来理解发言人的意图和对话的背景。仅凭“发言人2: 这是第一个”这一句话，信息量非常有限，无法确定是否存在诈骗行为。\n",
      "\n",
      "因此，基于提供的信息，我无法判断这段对话是否包含诈骗风险。正确的JSON输出表示为：\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"is_fraud\": null\n",
      "}\n",
      "```\n",
      "\n",
      "这表示没有足够的信息来确定是否涉及诈骗。在实际应用中，可能需要更详细的对话内容、上下文或使用机器学习模型来分析此类情况。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress:  66%|██████▌   | 1544/2348 [02:05<02:59,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid json: 为了提供准确的分析，我需要完整的对话内容。然而，根据你提供的信息片段 \"发言人10: 那\"，这个句子本身并不包含足够的上下文来判断是否存在诈骗风险。通常分析对话中的诈骗风险需要考虑整个对话的语境、涉及的交易细节、请求的敏感性（如个人信息、财务信息等）以及任何不寻常或紧迫的陈述。\n",
      "\n",
      "由于缺乏完整对话，我无法做出准确判断。因此，基于提供的信息，我将答案设置为：\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"is_fraud\": null\n",
      "}\n",
      "```\n",
      "\n",
      "这表示没有足够的信息来确定对话是否涉及诈骗。在实际情况下，建议完整查看对话内容并结合其他相关信息（如对话参与者的行为模式、使用的平台等）来进行更全面的风险评估。\n",
      "invalid json: 为了准确地评估对话内容是否存在诈骗风险，我们需要更多的上下文信息来理解发言人的背景、对话的语境以及他们讨论的具体内容。仅凭“发言人3: 明白。好的。”这一句话，我们无法确定是否存在诈骗行为，因为这句话本身并不包含任何明显的欺诈意图或信息。\n",
      "\n",
      "因此，基于提供的信息，合理的判断结果是：\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"is_fraud\": false\n",
      "}\n",
      "```\n",
      "\n",
      "请注意，这只是一个基于有限信息的假设性判断。在实际情况下，可能需要更多的情境细节来进行更准确的风险评估。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress:  74%|███████▍  | 1744/2348 [02:21<01:26,  6.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid json: 为了准确地评估对话内容是否存在诈骗风险，我们需要更多的上下文信息。仅凭“发言人1: 带来极大的助益。”这一句话，无法判断是否存在诈骗行为，因为这句话本身并不包含任何具体的、可能构成欺诈的信息。\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"is_fraud\": false\n",
      "}\n",
      "```\n",
      "\n",
      "请注意，如果后续出现了更多对话内容，可能需要重新评估整个对话的上下文以确定是否存在诈骗风险。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress:  80%|████████  | 1880/2348 [02:30<00:36, 12.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid json: 为了提供准确的分析结果，我需要看到具体的对话内容。请提供对话文本，然后我可以帮助您分析是否存在诈骗风险。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress:  93%|█████████▎| 2184/2348 [02:54<00:24,  6.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid json: 为了提供准确的分析，我需要完整的对话上下文。然而，根据给出的片段 \"发言人4: 就是\"，信息量非常有限，不足以判断是否存在诈骗风险。通常，诈骗对话会包含虚假信息、紧急要求转账、个人信息索取等特征。由于没有足够的上下文信息，我无法做出判断。\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"is_fraud\": null\n",
      "}\n",
      "```\n",
      "\n",
      "请注意，完整对话上下文对于此类分析至关重要。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress:  94%|█████████▎| 2200/2348 [02:59<00:36,  4.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid json: 为了提供准确的分析，我需要完整的对话内容。然而，根据你提供的信息片段 \"发言人10: 那\"，这个句子本身并不包含足够的上下文来判断是否存在诈骗风险。通常分析对话中的诈骗风险需要考虑整个对话的语境、涉及的交易细节、请求或提供的信息类型（如个人信息、财务信息等）以及任何紧迫性或压力性的语言使用。\n",
      "\n",
      "由于缺乏完整对话，我无法做出准确判断。因此，基于提供的信息片段，我将答案设置为：\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"is_fraud\": null\n",
      "}\n",
      "```\n",
      "\n",
      "这表示没有足够的信息来确定对话是否涉及诈骗。在实际情况下，建议完整查看对话内容并结合其他相关信息（如对话来源、目的、参与者的背景等）来进行更全面的风险评估。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress:  97%|█████████▋| 2280/2348 [03:06<00:08,  7.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid json: 为了提供准确的分析，我需要完整的对话上下文。仅凭“发言人2: 其中。”这一句话，信息量不足，无法判断是否存在诈骗风险。因此，根据提供的信息，输出的JSON结果为：\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"is_fraud\": null\n",
      "}\n",
      "```\n",
      "\n",
      "这表示基于当前信息，无法确定是否涉及诈骗。需要更多的对话内容来进行分析。\n",
      "invalid json: 为了准确地判断这段对话是否涉及诈骗风险，我们需要更多的上下文信息来理解发言人的背景、意图以及对话的语境。仅凭这段对话内容，我们无法确定是否存在诈骗风险。因此，基于提供的信息，我将输出的结果为：\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"is_fraud\": null\n",
      "}\n",
      "```\n",
      "\n",
      "这表示根据当前的信息，无法判断这段对话是否存在诈骗风险。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress:  99%|█████████▉| 2328/2348 [03:11<00:03,  6.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid json: 为了准确地判断这段对话是否存在诈骗风险，我们需要更多的上下文信息。仅凭这段对话内容“小张: 那他们是怎么操作的呢？”很难判断是否存在诈骗行为。这段对话可能是在讨论某个项目的执行流程、某个产品的使用方法，或者是对某个过程的疑问。\n",
      "\n",
      "因此，基于提供的信息，合理的判断结果是：\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"is_fraud\": false\n",
      "}\n",
      "```\n",
      "\n",
      "请注意，这只是一个基于当前信息的假设判断，并且在实际情况下，需要更多的情境和对话内容来进行更准确的风险评估。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress: 100%|██████████| 2348/2348 [03:13<00:00, 12.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn：1108, fp:57, fn:582, tp:601\n",
      "precision: 0.9133738601823708, recall: 0.5080304311073541, accuracy: 0.7278534923339012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%run evaluate.py\n",
    "model_path = '/data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-7B-Instruct'\n",
    "evaluate(model_path, '', testdata_path, device, batch=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6891d80b-5eff-4e74-b086-b9785b3027a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
