{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2181f2e-8394-4763-ae01-dff717010041",
   "metadata": {},
   "source": [
    "## 前言\n",
    "\n",
    "LLama.cpp是由Georgi Gerganov开发的一个开源工具，主要用于将大语言模型（LLM）转换为C++代码，使它们可以在任意的CPU设备上运行。\n",
    "\n",
    "它的优势在于：\n",
    "- 无需依赖pytorch和python，而是以c++编译的可执行文件来运行。\n",
    "- 支持丰富的硬件设备，包括Nvidia、AMD、Intel、Apple Silicon、华为昇腾等芯片。\n",
    "- 支持f16和f32混合精度，也支持8位、4位甚至1位的量化来加快推理。\n",
    "- 无需GPU，可只用CPU运行，甚至可以在Android设备上运行。\n",
    "\n",
    "本文我们将用llama.cpp来运行之前微调过的欺诈文本分类模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d457dbe-1880-4d48-a60e-2e4ba493e5c3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d50a29d-9d38-4f65-b5ac-bcdd315a7467",
   "metadata": {},
   "source": [
    "使用llama-server运行gguf, 并通过`http://192.168.31.200:8080/`来访问 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2b8d7ef-ee5b-4b8e-9b6d-edb15d7c67e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: no usable GPU found, --gpu-layers option will be ignored\n",
      "warning: one possible reason is that llama.cpp was compiled without GPU support\n",
      "warning: consult docs/build.md for compilation instructions\n",
      "build: 4893 (f4c3dd5d) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "system info: n_threads = 64, n_threads_batch = 64, total_threads = 128\n",
      "\n",
      "system_info: n_threads = 64 (n_threads_batch = 64) / 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "main: HTTP server is listening, hostname: 0.0.0.0, port: 8080, http threads: 127\n",
      "main: loading model\n",
      "srv    load_model: loading model '/data2/anti_fraud/models/anti_fraud_v11/model_bf16.gguf'\n",
      "llama_model_loader: loaded meta data with 28 key-value pairs and 338 tensors from /data2/anti_fraud/models/anti_fraud_v11/model_bf16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = qwen2\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = 1___5B-Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Qwen2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 1.5B\n",
      "llama_model_loader: - kv   6:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   7:                               general.tags arr[str,2]       = [\"chat\", \"text-generation\"]\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv   9:                          qwen2.block_count u32              = 28\n",
      "llama_model_loader: - kv  10:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     qwen2.embedding_length u32              = 1536\n",
      "llama_model_loader: - kv  12:                  qwen2.feed_forward_length u32              = 8960\n",
      "llama_model_loader: - kv  13:                 qwen2.attention.head_count u32              = 12\n",
      "llama_model_loader: - kv  14:              qwen2.attention.head_count_kv u32              = 2\n",
      "llama_model_loader: - kv  15:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  16:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\n",
      "llama_model_loader: - kv  26:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  27:                          general.file_type u32              = 1\n",
      "llama_model_loader: - type  f32:  141 tensors\n",
      "llama_model_loader: - type  f16:  196 tensors\n",
      "llama_model_loader: - type bf16:    1 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = F16\n",
      "print_info: file size   = 2.88 GiB (16.00 BPW) \n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.9308 MB\n",
      "print_info: arch             = qwen2\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 1536\n",
      "print_info: n_layer          = 28\n",
      "print_info: n_head           = 12\n",
      "print_info: n_head_kv        = 2\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 6\n",
      "print_info: n_embd_k_gqa     = 256\n",
      "print_info: n_embd_v_gqa     = 256\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 8960\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 1.5B\n",
      "print_info: model params     = 1.54 B\n",
      "print_info: general.name     = qwen2\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 151936\n",
      "print_info: n_merges         = 151387\n",
      "print_info: BOS token        = 151643 '<|endoftext|>'\n",
      "print_info: EOS token        = 151645 '<|im_end|>'\n",
      "print_info: EOT token        = 151645 '<|im_end|>'\n",
      "print_info: PAD token        = 151643 '<|endoftext|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 151643 '<|endoftext|>'\n",
      "print_info: EOG token        = 151645 '<|im_end|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors:   CPU_Mapped model buffer size =  2944.68 MiB\n",
      "......................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 2048\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 1\n",
      "llama_context: freq_base     = 1000000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_context:        CPU  output buffer size =     0.58 MiB\n",
      "init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1\n",
      "init:        CPU KV buffer size =   112.00 MiB\n",
      "llama_context: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB\n",
      "llama_context:        CPU compute buffer size =   303.78 MiB\n",
      "llama_context: graph nodes  = 875\n",
      "llama_context: graph splits = 1\n",
      "common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\n",
      "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
      "srv          init: initializing slots, n_slots = 1\n",
      "slot         init: id  0 | task -1 | new slot n_ctx_slot = 4096\n",
      "main: model loaded\n",
      "main: chat template, chat_template: {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}, example_format: '<|im_start|>system\n",
      "You are a helpful assistant<|im_end|>\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "srv  log_server_r: request: GET /v1/chat/completions 127.0.0.1 404\n",
      "^C\n",
      "srv    operator(): operator(): cleaning up before exit...\n",
      "terminate called without an active exception\n"
     ]
    }
   ],
   "source": [
    "!/root/autodl-tmp/llama.cpp/build/bin/llama-server -m /data2/anti_fraud/models/anti_fraud_v11/model_bf16.gguf -ngl 28 -fa --host 0.0.0.0 --port 8080\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b643504e-10c3-4790-8258-599a2fae376f",
   "metadata": {},
   "source": [
    "## 模型文件转换\n",
    "我们微调后的模型由两部分组成：基座模型和Lora适配器，需要对这两者分别转换，最后再合并。\n",
    "\n",
    "先用`convert_hf_to_gguf.py`工具转换基座模型。\n",
    "\n",
    "> 注：convert_hf_to_gguf.py是llama.cpp提供的工具脚本，位于安装目录下，用于将huggingface上下载的safetensors模型格式转换为gguf文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79dd7f49-7f34-46b9-aa0b-beb7dbdf70f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: Qwen2-1___5B-Instruct\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> BF16, shape = {1536, 151936}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight, torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight, torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight, torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,      torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight, torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,      torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight, torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,      torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight, torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,      torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight, torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,      torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight, torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,      torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight, torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,      torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight, torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,      torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight, torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,      torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight, torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.bias,        torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,      torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.bias,        torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,      torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {8960, 1536}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {1536, 8960}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.bias,         torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.bfloat16 --> BF16, shape = {1536, 1536}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.bias,         torch.bfloat16 --> F32, shape = {256}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.bfloat16 --> BF16, shape = {1536, 256}\n",
      "INFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {1536}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 32768\n",
      "INFO:hf-to-gguf:gguf: embedding length = 1536\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 8960\n",
      "INFO:hf-to-gguf:gguf: head count = 12\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 2\n",
      "INFO:hf-to-gguf:gguf: rope theta = 1000000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\n",
      "INFO:hf-to-gguf:gguf: file type = 32\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Adding 151387 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type eos to 151645\n",
      "INFO:gguf.vocab:Setting special token type pad to 151643\n",
      "INFO:gguf.vocab:Setting special token type bos to 151643\n",
      "INFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:/data2/anti_fraud/models/anti_fraud_v11/qwen2_bf16.gguf: n_tensors = 338, total_size = 3.1G\n",
      "Writing: 100%|██████████████████████████| 3.09G/3.09G [00:57<00:00, 53.5Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to /data2/anti_fraud/models/anti_fraud_v11/qwen2_bf16.gguf\n"
     ]
    }
   ],
   "source": [
    "!python /data2/downloads/llama.cpp/convert_hf_to_gguf.py \\\n",
    "    --outtype bf16 \\\n",
    "    --outfile /data2/anti_fraud/models/anti_fraud_v11/qwen2_bf16.gguf \\\n",
    "    --model-name qwen2 \\\n",
    "    /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3198e658-f3c0-41a0-94b0-768c31d59e0d",
   "metadata": {},
   "source": [
    "接下来使用`convert_lora_to_gguf.py `脚本工具来转换lora适配器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c2b9142-5206-4c21-954b-c4e491de41c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:lora-to-gguf:Loading base model: Qwen2-1___5B-Instruct\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:lora-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight.lora_a, torch.float32 --> F32, shape = {8960, 16}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight.lora_b, torch.float32 --> F32, shape = {16, 8960}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight.lora_b, torch.float32 --> F32, shape = {16, 1536}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight.lora_a, torch.float32 --> F32, shape = {1536, 16}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight.lora_b, torch.float32 --> F32, shape = {16, 256}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:/data2/anti_fraud/models/anti_fraud_v11/lora_0913_4_bf16.gguf: n_tensors = 392, total_size = 73.9M\n",
      "Writing: 100%|██████████████████████████| 73.9M/73.9M [00:05<00:00, 14.5Mbyte/s]\n",
      "INFO:lora-to-gguf:Model successfully exported to /data2/anti_fraud/models/anti_fraud_v11/lora_0913_4_bf16.gguf\n"
     ]
    }
   ],
   "source": [
    "!python /data2/downloads/llama.cpp/convert_lora_to_gguf.py \\\n",
    "    --base /data2/anti_fraud/models/modelscope/hub/Qwen/Qwen2-1___5B-Instruct \\\n",
    "    --outfile /data2/anti_fraud/models/anti_fraud_v11/lora_0913_4_bf16.gguf \\\n",
    "    /data2/anti_fraud/models/Qwen2-1___5B-Instruct_ft_0830_2/checkpoint-1900 \\\n",
    "    --outtype bf16 --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f0f349-4a82-46e3-869c-b7b83e9e8eab",
   "metadata": {},
   "source": [
    "执行完后，得到一个Lora适配器的gguf文件`lora_0913_4_bf16.gguf`。\n",
    "\n",
    "使用`llama-export-lora`工具将基座模型和Lora适配器合并为一个gguf文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e4b82c3-8c95-482f-8fae-96c1ec767a18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_input: loaded gguf from /data2/anti_fraud/models/anti_fraud_v11/qwen2_bf16.gguf\n",
      "file_input: loaded gguf from /data2/anti_fraud/models/anti_fraud_v11/lora_0913_4_bf16.gguf\n",
      "copy_tensor :  blk.0.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.0.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.0.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.0.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.0.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.0.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.0.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.0.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.0.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.0.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.0.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.0.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.1.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.1.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.1.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.1.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.1.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.1.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.1.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.1.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.1.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.1.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.1.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.1.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.10.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.10.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.10.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.10.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.10.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.10.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.10.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.10.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.10.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.10.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.10.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.10.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.11.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.11.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.11.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.11.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.11.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.11.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.11.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.11.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.11.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.11.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.11.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.11.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.12.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.12.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.12.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.12.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.12.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.12.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.12.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.12.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.12.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.12.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.12.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.12.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.13.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.13.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.13.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.13.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.13.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.13.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.13.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.13.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.13.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.13.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.13.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.13.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.14.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.14.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.14.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.14.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.14.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.14.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.14.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.14.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.14.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.14.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.14.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.14.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.15.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.15.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.15.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.15.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.15.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.15.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.15.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.15.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.15.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.15.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.15.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.15.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.16.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.16.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.16.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.16.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.16.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.16.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.16.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.16.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.16.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.16.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.16.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.16.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.17.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.17.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.17.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.17.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.17.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.17.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.17.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.17.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.17.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.17.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.17.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.17.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.18.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.18.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.18.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.18.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.18.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.18.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.18.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.18.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.18.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.18.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.18.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.18.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.19.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.19.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.19.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.19.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.19.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.19.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.19.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.19.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.19.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.19.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.19.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.19.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.2.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.2.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.2.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.2.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.2.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.2.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.2.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.2.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.2.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.2.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.2.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.2.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.20.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.20.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.20.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.20.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.20.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.20.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.20.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.20.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.20.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.20.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.20.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.20.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.21.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.21.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.21.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.21.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.21.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.21.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.21.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.21.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.21.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.21.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.21.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.21.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.22.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.22.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.22.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.22.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.22.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.22.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.22.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.22.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.22.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.22.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.22.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.22.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.23.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.23.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.23.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.23.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.23.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.23.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.23.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.23.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.23.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.23.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.23.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.23.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.24.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.24.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.24.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.24.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.24.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.24.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.24.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.24.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.24.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.24.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.24.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.24.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.25.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.25.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.25.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.25.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.25.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.25.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.25.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.25.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.25.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.25.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.25.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.25.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.26.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.26.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.26.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.26.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.26.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.26.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.26.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.26.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.26.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.26.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.26.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.26.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.27.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.27.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.27.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.27.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.27.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.27.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.27.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.27.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.27.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.27.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.27.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.27.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.3.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.3.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.3.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.3.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.3.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.3.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.3.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.3.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.3.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.3.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.3.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.3.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.4.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.4.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.4.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.4.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.4.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.4.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.4.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.4.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.4.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.4.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.4.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.4.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.5.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.5.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.5.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.5.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.5.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.5.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.5.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.5.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.5.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.5.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.5.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.5.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.6.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.6.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.6.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.6.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.6.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.6.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.6.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.6.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.6.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.6.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.6.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.6.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.7.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.7.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.7.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.7.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.7.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.7.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.7.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.7.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.7.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.7.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.7.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.7.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.8.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.8.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.8.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.8.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.8.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.8.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.8.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.8.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.8.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.8.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.8.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.8.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.9.attn_k.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.9.attn_k.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.9.attn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.9.attn_output.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.9.attn_q.bias [1536, 1, 1, 1]\n",
      "merge_tensor : blk.9.attn_q.weight [1536, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.9.attn_v.bias [256, 1, 1, 1]\n",
      "merge_tensor : blk.9.attn_v.weight [1536, 256, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.9.ffn_down.weight [8960, 1536, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "merge_tensor : blk.9.ffn_gate.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  blk.9.ffn_norm.weight [1536, 1, 1, 1]\n",
      "merge_tensor : blk.9.ffn_up.weight [1536, 8960, 1, 1]\n",
      "merge_tensor :   + dequantize base tensor from bf16 to F32\n",
      "merge_tensor :   + merging from adapter[0] type=f32\n",
      "merge_tensor :     input_scale=1.000000 calculated_scale=2.000000 rank=16\n",
      "merge_tensor :   + output type is f16\n",
      "copy_tensor :  output_norm.weight [1536, 1, 1, 1]\n",
      "copy_tensor :  token_embd.weight [1536, 151936, 1, 1]\n",
      "run_merge : merged 196 tensors with lora adapters\n",
      "run_merge : wrote 338 tensors to output file\n",
      "done, output file is /data2/anti_fraud/models/anti_fraud_v11/model_bf16.gguf\n"
     ]
    }
   ],
   "source": [
    "!/data2/downloads/llama.cpp/llama-export-lora \\\n",
    "    -m /data2/anti_fraud/models/anti_fraud_v11/qwen2_bf16.gguf \\\n",
    "    -o /data2/anti_fraud/models/anti_fraud_v11/model_bf16.gguf \\\n",
    "    --lora /data2/anti_fraud/models/anti_fraud_v11/lora_0913_4_bf16.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6060b063-501e-423c-888e-b85c14720483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 6114481\n",
      "drwxr-xr-x 2 root root       4096 Mar 16 23:39 .\n",
      "drwxr-xr-x 7 root root       4096 Mar 16 23:33 ..\n",
      "-rw-r--r-- 1 root root   73886368 Mar 16 23:37 lora_0913_4_bf16.gguf\n",
      "-rw-r--r-- 1 root root 3093666720 Mar 16 23:40 model_bf16.gguf\n",
      "-rw-r--r-- 1 root root 3093666720 Mar 16 23:35 qwen2_bf16.gguf\n"
     ]
    }
   ],
   "source": [
    "!ls -al /data2/anti_fraud/models/anti_fraud_v11/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a39e4f-114d-43e5-96ff-aa36e96bd02b",
   "metadata": {},
   "source": [
    "\n",
    "查看导出的文件：\n",
    "\n",
    "```python\n",
    "-rw-rw-r-- 1   42885408 Nov  9 14:57 lora_0913_4_bf16.gguf\n",
    "-rw-rw-r-- 1 3093666720 Nov  9 14:58 model_bf16.gguf\n",
    "-rw-rw-r-- 1 3093666720 Nov  9 14:56 qwen2_bf16.gguf\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfe4981-736d-453a-a3c7-20a78dcde858",
   "metadata": {},
   "source": [
    "经过上面三步，我们就将safetensors格式的基座模型和lora适配器导出为gguf格式的模型文件`model_bf16.gguf`，此时模型文件大小并没有变化，仍然有3G。\n",
    "\n",
    "用`llama-cli`命令验证此模型文件是否能正常work。\n",
    "> llama-cli是一种命令行接口，允许用户只通过一条命令完成模型启动和模型访问，用于快速测试和调试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6db20090-9c0d-4652-ab5f-982788ad2791",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build: 4893 (f4c3dd5d) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: llama backend init\n",
      "main: load the model and apply lora adapter, if any\n",
      "llama_model_loader: loaded meta data with 28 key-value pairs and 338 tensors from /data2/anti_fraud/models/anti_fraud_v11/model_bf16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = qwen2\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = 1___5B-Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Qwen2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 1.5B\n",
      "llama_model_loader: - kv   6:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   7:                               general.tags arr[str,2]       = [\"chat\", \"text-generation\"]\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv   9:                          qwen2.block_count u32              = 28\n",
      "llama_model_loader: - kv  10:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     qwen2.embedding_length u32              = 1536\n",
      "llama_model_loader: - kv  12:                  qwen2.feed_forward_length u32              = 8960\n",
      "llama_model_loader: - kv  13:                 qwen2.attention.head_count u32              = 12\n",
      "llama_model_loader: - kv  14:              qwen2.attention.head_count_kv u32              = 2\n",
      "llama_model_loader: - kv  15:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  16:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\n",
      "llama_model_loader: - kv  26:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  27:                          general.file_type u32              = 1\n",
      "llama_model_loader: - type  f32:  141 tensors\n",
      "llama_model_loader: - type  f16:  196 tensors\n",
      "llama_model_loader: - type bf16:    1 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = F16\n",
      "print_info: file size   = 2.88 GiB (16.00 BPW) \n",
      "load: special tokens cache size = 3\n",
      "load: token to piece cache size = 0.9308 MB\n",
      "print_info: arch             = qwen2\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 1536\n",
      "print_info: n_layer          = 28\n",
      "print_info: n_head           = 12\n",
      "print_info: n_head_kv        = 2\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 6\n",
      "print_info: n_embd_k_gqa     = 256\n",
      "print_info: n_embd_v_gqa     = 256\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 8960\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 1.5B\n",
      "print_info: model params     = 1.54 B\n",
      "print_info: general.name     = qwen2\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 151936\n",
      "print_info: n_merges         = 151387\n",
      "print_info: BOS token        = 151643 '<|endoftext|>'\n",
      "print_info: EOS token        = 151645 '<|im_end|>'\n",
      "print_info: EOT token        = 151645 '<|im_end|>'\n",
      "print_info: PAD token        = 151643 '<|endoftext|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 151643 '<|endoftext|>'\n",
      "print_info: EOG token        = 151645 '<|im_end|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors:   CPU_Mapped model buffer size =  2944.68 MiB\n",
      "......................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 2048\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 1000000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_context:        CPU  output buffer size =     0.58 MiB\n",
      "init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1\n",
      "init:        CPU KV buffer size =   112.00 MiB\n",
      "llama_context: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB\n",
      "llama_context:        CPU compute buffer size =   303.76 MiB\n",
      "llama_context: graph nodes  = 986\n",
      "llama_context: graph splits = 1\n",
      "common_init_from_params: setting dry_penalty_last_n to ctx_size = 4096\n",
      "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
      "main: llama threadpool init, n_threads = 64\n",
      "\n",
      "system_info: n_threads = 64 (n_threads_batch = 64) / 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "\n",
      "sampler seed: 4179162958\n",
      "sampler params: \n",
      "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
      "\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 4096\n",
      "\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800\n",
      "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
      "sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \n",
      "generate: n_ctx = 4096, n_batch = 2048, n_predict = 100, n_keep = 0\n",
      "\n",
      "我是一个来自太行山下小村庄家的孩子，名叫郭宝山。我的家住在太行山脚下，村子里有几十户人家，我们家住的是老房子，家里有三口人，爷爷、奶奶和我。\n",
      "有一天，我正在家里玩，突然，我看到有位老人在打水，我想上去帮忙，但是我又怕他会骂我，于是我就在原地不动。过了一会儿，那位老人走了，我这才从原地站起来，我心想：我一定要帮爷爷奶奶和那位\n",
      "\n",
      "llama_perf_sampler_print:    sampling time =     200.26 ms /   111 runs   (    1.80 ms per token,   554.29 tokens per second)\n",
      "llama_perf_context_print:        load time =     753.66 ms\n",
      "llama_perf_context_print: prompt eval time =     482.89 ms /    11 tokens (   43.90 ms per token,    22.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =   25137.48 ms /    99 runs   (  253.91 ms per token,     3.94 tokens per second)\n",
      "llama_perf_context_print:       total time =   25901.90 ms /   110 tokens\n"
     ]
    }
   ],
   "source": [
    "!/data2/downloads/llama.cpp/llama-cli -no-cnv \\\n",
    "\t-m /data2/anti_fraud/models/anti_fraud_v11/model_bf16.gguf \\\n",
    "\t-p \"我是一个来自太行山下小村庄家的孩子\" \\\n",
    "\t-n 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a94d55-1cc5-44d8-a9be-f9ac802061ea",
   "metadata": {},
   "source": [
    "## 量化\n",
    "\n",
    "使用`llama-quantize`工具将模型文件由16位量化为8位。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48d79498-b525-4547-9e17-745deb3b3837",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 4893 (f4c3dd5d)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing '/data2/anti_fraud/models/anti_fraud_v11/model_bf16.gguf' to '/data2/anti_fraud/models/anti_fraud_v11/model_bf16_q8_0.gguf' as Q8_0\n",
      "llama_model_loader: loaded meta data with 28 key-value pairs and 338 tensors from /data2/anti_fraud/models/anti_fraud_v11/model_bf16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = qwen2\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = 1___5B-Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Qwen2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 1.5B\n",
      "llama_model_loader: - kv   6:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   7:                               general.tags arr[str,2]       = [\"chat\", \"text-generation\"]\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv   9:                          qwen2.block_count u32              = 28\n",
      "llama_model_loader: - kv  10:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv  11:                     qwen2.embedding_length u32              = 1536\n",
      "llama_model_loader: - kv  12:                  qwen2.feed_forward_length u32              = 8960\n",
      "llama_model_loader: - kv  13:                 qwen2.attention.head_count u32              = 12\n",
      "llama_model_loader: - kv  14:              qwen2.attention.head_count_kv u32              = 2\n",
      "llama_model_loader: - kv  15:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  16:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  22:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  23:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\n",
      "llama_model_loader: - kv  26:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  27:                          general.file_type u32              = 1\n",
      "llama_model_loader: - type  f32:  141 tensors\n",
      "llama_model_loader: - type  f16:  196 tensors\n",
      "llama_model_loader: - type bf16:    1 tensors\n",
      "[   1/ 338]                   output_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[   2/ 338]                    token_embd.weight - [ 1536, 151936,     1,     1], type =   bf16, converting to q8_0 .. size =   445.12 MiB ->   236.47 MiB\n",
      "[   3/ 338]                    blk.0.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[   4/ 338]                  blk.0.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[   5/ 338]               blk.0.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[   6/ 338]             blk.0.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[   7/ 338]                    blk.0.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[   8/ 338]                  blk.0.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[   9/ 338]                    blk.0.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[  10/ 338]                  blk.0.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[  11/ 338]                blk.0.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  12/ 338]                blk.0.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  13/ 338]                blk.0.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  14/ 338]                  blk.0.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  15/ 338]                    blk.1.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[  16/ 338]                  blk.1.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[  17/ 338]               blk.1.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  18/ 338]             blk.1.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  19/ 338]                    blk.1.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  20/ 338]                  blk.1.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  21/ 338]                    blk.1.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[  22/ 338]                  blk.1.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[  23/ 338]                blk.1.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  24/ 338]                blk.1.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  25/ 338]                blk.1.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  26/ 338]                  blk.1.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  27/ 338]                    blk.2.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[  28/ 338]                  blk.2.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[  29/ 338]               blk.2.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  30/ 338]             blk.2.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  31/ 338]                    blk.2.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  32/ 338]                  blk.2.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  33/ 338]                    blk.2.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[  34/ 338]                  blk.2.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[  35/ 338]                blk.2.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  36/ 338]                blk.2.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  37/ 338]                blk.2.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  38/ 338]                  blk.2.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  39/ 338]                    blk.3.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[  40/ 338]                  blk.3.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[  41/ 338]               blk.3.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  42/ 338]             blk.3.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  43/ 338]                    blk.3.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  44/ 338]                  blk.3.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  45/ 338]                    blk.3.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[  46/ 338]                  blk.3.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[  47/ 338]                blk.3.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  48/ 338]                blk.3.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  49/ 338]                blk.3.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  50/ 338]                  blk.3.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  51/ 338]                    blk.4.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[  52/ 338]                  blk.4.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[  53/ 338]               blk.4.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  54/ 338]             blk.4.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  55/ 338]                    blk.4.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  56/ 338]                  blk.4.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  57/ 338]                    blk.4.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[  58/ 338]                  blk.4.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[  59/ 338]                blk.4.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  60/ 338]                blk.4.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  61/ 338]                blk.4.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  62/ 338]                  blk.4.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  63/ 338]                    blk.5.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[  64/ 338]                  blk.5.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[  65/ 338]               blk.5.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  66/ 338]             blk.5.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  67/ 338]                    blk.5.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  68/ 338]                  blk.5.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  69/ 338]                    blk.5.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[  70/ 338]                  blk.5.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[  71/ 338]                blk.5.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  72/ 338]                blk.5.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  73/ 338]                blk.5.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  74/ 338]                  blk.5.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  75/ 338]                    blk.6.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[  76/ 338]                  blk.6.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[  77/ 338]               blk.6.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  78/ 338]             blk.6.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  79/ 338]                    blk.6.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  80/ 338]                  blk.6.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  81/ 338]                    blk.6.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[  82/ 338]                  blk.6.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[  83/ 338]                blk.6.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  84/ 338]                blk.6.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  85/ 338]                blk.6.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  86/ 338]                  blk.6.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  87/ 338]                    blk.7.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[  88/ 338]                  blk.7.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[  89/ 338]               blk.7.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  90/ 338]             blk.7.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  91/ 338]                    blk.7.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  92/ 338]                  blk.7.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[  93/ 338]                    blk.7.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[  94/ 338]                  blk.7.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[  95/ 338]                blk.7.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  96/ 338]                blk.7.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  97/ 338]                blk.7.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[  98/ 338]                  blk.7.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[  99/ 338]                    blk.8.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 100/ 338]                  blk.8.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 101/ 338]               blk.8.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 102/ 338]             blk.8.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 103/ 338]                    blk.8.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 104/ 338]                  blk.8.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 105/ 338]                    blk.8.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 106/ 338]                  blk.8.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 107/ 338]                blk.8.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 108/ 338]                blk.8.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 109/ 338]                blk.8.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 110/ 338]                  blk.8.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 111/ 338]                    blk.9.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 112/ 338]                  blk.9.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 113/ 338]               blk.9.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 114/ 338]             blk.9.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 115/ 338]                    blk.9.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 116/ 338]                  blk.9.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 117/ 338]                    blk.9.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 118/ 338]                  blk.9.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 119/ 338]                blk.9.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 120/ 338]                blk.9.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 121/ 338]                blk.9.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 122/ 338]                  blk.9.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 123/ 338]                   blk.10.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 124/ 338]                 blk.10.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 125/ 338]              blk.10.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 126/ 338]            blk.10.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 127/ 338]                   blk.10.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 128/ 338]                 blk.10.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 129/ 338]                   blk.10.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 130/ 338]                 blk.10.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 131/ 338]               blk.10.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 132/ 338]               blk.10.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 133/ 338]               blk.10.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 134/ 338]                 blk.10.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 135/ 338]                   blk.11.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 136/ 338]                 blk.11.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 137/ 338]              blk.11.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 138/ 338]            blk.11.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 139/ 338]                   blk.11.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 140/ 338]                 blk.11.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 141/ 338]                   blk.11.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 142/ 338]                 blk.11.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 143/ 338]               blk.11.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 144/ 338]               blk.11.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 145/ 338]               blk.11.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 146/ 338]                 blk.11.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 147/ 338]                   blk.12.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 148/ 338]                 blk.12.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 149/ 338]              blk.12.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 150/ 338]            blk.12.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 151/ 338]                   blk.12.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 152/ 338]                 blk.12.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 153/ 338]                   blk.12.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 154/ 338]                 blk.12.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 155/ 338]               blk.12.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 156/ 338]               blk.12.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 157/ 338]               blk.12.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 158/ 338]                 blk.12.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 159/ 338]                   blk.13.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 160/ 338]                 blk.13.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 161/ 338]              blk.13.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 162/ 338]            blk.13.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 163/ 338]                   blk.13.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 164/ 338]                 blk.13.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 165/ 338]                   blk.13.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 166/ 338]                 blk.13.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 167/ 338]               blk.13.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 168/ 338]               blk.13.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 169/ 338]               blk.13.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 170/ 338]                 blk.13.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 171/ 338]                   blk.14.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 172/ 338]                 blk.14.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 173/ 338]              blk.14.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 174/ 338]            blk.14.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 175/ 338]                   blk.14.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 176/ 338]                 blk.14.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 177/ 338]                   blk.14.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 178/ 338]                 blk.14.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 179/ 338]               blk.14.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 180/ 338]               blk.14.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 181/ 338]               blk.14.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 182/ 338]                 blk.14.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 183/ 338]                   blk.15.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 184/ 338]                 blk.15.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 185/ 338]              blk.15.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 186/ 338]            blk.15.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 187/ 338]                   blk.15.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 188/ 338]                 blk.15.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 189/ 338]                   blk.15.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 190/ 338]                 blk.15.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 191/ 338]               blk.15.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 192/ 338]               blk.15.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 193/ 338]               blk.15.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 194/ 338]                 blk.15.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 195/ 338]                   blk.16.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 196/ 338]                 blk.16.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 197/ 338]              blk.16.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 198/ 338]            blk.16.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 199/ 338]                   blk.16.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 200/ 338]                 blk.16.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 201/ 338]                   blk.16.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 202/ 338]                 blk.16.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 203/ 338]               blk.16.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 204/ 338]               blk.16.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 205/ 338]               blk.16.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 206/ 338]                 blk.16.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 207/ 338]                   blk.17.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 208/ 338]                 blk.17.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 209/ 338]              blk.17.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 210/ 338]            blk.17.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 211/ 338]                   blk.17.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 212/ 338]                 blk.17.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 213/ 338]                   blk.17.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 214/ 338]                 blk.17.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 215/ 338]               blk.17.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 216/ 338]               blk.17.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 217/ 338]               blk.17.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 218/ 338]                 blk.17.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 219/ 338]                   blk.18.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 220/ 338]                 blk.18.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 221/ 338]              blk.18.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 222/ 338]            blk.18.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 223/ 338]                   blk.18.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 224/ 338]                 blk.18.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 225/ 338]                   blk.18.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 226/ 338]                 blk.18.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 227/ 338]               blk.18.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 228/ 338]               blk.18.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 229/ 338]               blk.18.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 230/ 338]                 blk.18.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 231/ 338]                   blk.19.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 232/ 338]                 blk.19.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 233/ 338]              blk.19.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 234/ 338]            blk.19.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 235/ 338]                   blk.19.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 236/ 338]                 blk.19.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 237/ 338]                   blk.19.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 238/ 338]                 blk.19.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 239/ 338]               blk.19.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 240/ 338]               blk.19.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 241/ 338]               blk.19.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 242/ 338]                 blk.19.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 243/ 338]                   blk.20.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 244/ 338]                 blk.20.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 245/ 338]              blk.20.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 246/ 338]            blk.20.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 247/ 338]                   blk.20.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 248/ 338]                 blk.20.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 249/ 338]                   blk.20.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 250/ 338]                 blk.20.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 251/ 338]               blk.20.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 252/ 338]               blk.20.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 253/ 338]               blk.20.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 254/ 338]                 blk.20.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 255/ 338]                   blk.21.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 256/ 338]                 blk.21.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 257/ 338]              blk.21.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 258/ 338]            blk.21.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 259/ 338]                   blk.21.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 260/ 338]                 blk.21.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 261/ 338]                   blk.21.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 262/ 338]                 blk.21.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 263/ 338]               blk.21.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 264/ 338]               blk.21.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 265/ 338]               blk.21.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 266/ 338]                 blk.21.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 267/ 338]                   blk.22.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 268/ 338]                 blk.22.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 269/ 338]              blk.22.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 270/ 338]            blk.22.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 271/ 338]                   blk.22.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 272/ 338]                 blk.22.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 273/ 338]                   blk.22.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 274/ 338]                 blk.22.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 275/ 338]               blk.22.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 276/ 338]               blk.22.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 277/ 338]               blk.22.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 278/ 338]                 blk.22.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 279/ 338]                   blk.23.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 280/ 338]                 blk.23.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 281/ 338]              blk.23.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 282/ 338]            blk.23.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 283/ 338]                   blk.23.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 284/ 338]                 blk.23.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 285/ 338]                   blk.23.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 286/ 338]                 blk.23.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 287/ 338]               blk.23.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 288/ 338]               blk.23.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 289/ 338]               blk.23.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 290/ 338]                 blk.23.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 291/ 338]                   blk.24.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 292/ 338]                 blk.24.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 293/ 338]              blk.24.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 294/ 338]            blk.24.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 295/ 338]                   blk.24.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 296/ 338]                 blk.24.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 297/ 338]                   blk.24.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 298/ 338]                 blk.24.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 299/ 338]               blk.24.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 300/ 338]               blk.24.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 301/ 338]               blk.24.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 302/ 338]                 blk.24.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 303/ 338]                   blk.25.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 304/ 338]                 blk.25.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 305/ 338]              blk.25.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 306/ 338]            blk.25.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 307/ 338]                   blk.25.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 308/ 338]                 blk.25.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 309/ 338]                   blk.25.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 310/ 338]                 blk.25.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 311/ 338]               blk.25.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 312/ 338]               blk.25.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 313/ 338]               blk.25.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 314/ 338]                 blk.25.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 315/ 338]                   blk.26.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 316/ 338]                 blk.26.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 317/ 338]              blk.26.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 318/ 338]            blk.26.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 319/ 338]                   blk.26.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 320/ 338]                 blk.26.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 321/ 338]                   blk.26.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 322/ 338]                 blk.26.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 323/ 338]               blk.26.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 324/ 338]               blk.26.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 325/ 338]               blk.26.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 326/ 338]                 blk.26.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 327/ 338]                   blk.27.attn_k.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 328/ 338]                 blk.27.attn_k.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 329/ 338]              blk.27.attn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 330/ 338]            blk.27.attn_output.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 331/ 338]                   blk.27.attn_q.bias - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 332/ 338]                 blk.27.attn_q.weight - [ 1536,  1536,     1,     1], type =    f16, converting to q8_0 .. size =     4.50 MiB ->     2.39 MiB\n",
      "[ 333/ 338]                   blk.27.attn_v.bias - [  256,     1,     1,     1], type =    f32, size =    0.001 MB\n",
      "[ 334/ 338]                 blk.27.attn_v.weight - [ 1536,   256,     1,     1], type =    f16, converting to q8_0 .. size =     0.75 MiB ->     0.40 MiB\n",
      "[ 335/ 338]               blk.27.ffn_down.weight - [ 8960,  1536,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 336/ 338]               blk.27.ffn_gate.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "[ 337/ 338]               blk.27.ffn_norm.weight - [ 1536,     1,     1,     1], type =    f32, size =    0.006 MB\n",
      "[ 338/ 338]                 blk.27.ffn_up.weight - [ 1536,  8960,     1,     1], type =    f16, converting to q8_0 .. size =    26.25 MiB ->    13.95 MiB\n",
      "llama_model_quantize_impl: model size  =  2944.68 MB\n",
      "llama_model_quantize_impl: quant size  =  1564.62 MB\n",
      "\n",
      "main: quantize time = 17326.90 ms\n",
      "main:    total time = 17326.90 ms\n"
     ]
    }
   ],
   "source": [
    "!/data2/downloads/llama.cpp/llama-quantize \\\n",
    "/data2/anti_fraud/models/anti_fraud_v11/model_bf16.gguf /data2/anti_fraud/models/anti_fraud_v11/model_bf16_q8_0.gguf q8_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f379cf2-b2f0-42f7-9942-5c8ac79a2310",
   "metadata": {},
   "source": [
    "经过量化后，模型文件由`2944.68MB`减小到`1564.62MB`，几乎缩小了一倍。\n",
    "\n",
    "## 转换为ollama文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdc9cb32-342c-4557-825a-a3463e907ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM ./model_bf16_q8_0.gguf\n",
      "\n",
      "TEMPLATE \"\"\"{% set system_message = 'You are a helpful assistant.' %}{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ '<|im_start|>system\\n' + system_message + '<|im_end|>\\n' }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\\n' + content + '<|im_end|>\\n<|im_start|>assistant\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|im_end|>' + '\\n' }}{% endif %}{% endfor %}\"\"\"\n",
      "\n",
      "PARAMETER stop \"<|im_end|>\""
     ]
    }
   ],
   "source": [
    "!cat /data2/anti_fraud/models/anti_fraud_v11/anti_fraud.modelfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63ae9ad-e901-4b33-9c84-4e19bade606e",
   "metadata": {},
   "source": [
    "> 注：ollama不支持bf16，相关报错信息：Error: invalid file magic\n",
    "[不支持bf16的说明](https://github.com/ollama/ollama/issues/4670)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d833fea5-e650-4cc1-a3a6-39934fd99137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:55e6d657392b53ef93b4840bf4b5f2b19047936a44da152e3caf5c2c8c120d7f 100% \u001b[K\n",
      "parsing GGUF ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:55e6d657392b53ef93b4840bf4b5f2b19047936a44da152e3caf5c2c8c120d7f 100% \u001b[K\n",
      "parsing GGUF \u001b[K\n",
      "using existing layer sha256:55e6d657392b53ef93b4840bf4b5f2b19047936a44da152e3caf5c2c8c120d7f \u001b[K\n",
      "using autodetected template chatml \u001b[K\n",
      "creating new layer sha256:f02dd72bb2423204352eabc5637b44d79d17f109fdb510a7c51455892aa2d216 \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama create qwen2:1.5b -f /data2/anti_fraud/models/anti_fraud_v11/anti_fraud.modelfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c64650-5e5c-4b6c-a48c-791007da479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "删除模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f41755-bb2f-40f1-b571-7698cc9cd0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama rm qwen2:1.5b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
